Gemini

Optimizing Parquet File Reading and Backtesting
Save as PDF

My Stuff
Image of 
Image of 
Image of 
Optimizing Parquet File Reading and Backtesting
Web Design Studio Landing Page
Three.js Web Agency Animation
I make this jewellery, and i need you to put this on a model and there should be proper enviroment and everything, this is brass silver jewellery - indian
Clean the background, dont change my face
Offering Assistance and Partnership
Lively Personality Traits Explained
Improving Communication: Energy and Vibe
Conversation with Gemini
This is one of my chat with chatgpt, i need improvement suggestions while keeping constraint of reading everything intact, so suggest better stuff :



I want to do some testing for benchmarking and spped testing for reading these files and stuff as later i want to do backtesting on these 85k files, instead of just 100 now, and i want you to do in this folder : Nice, this is a fun little benchmark mission üß™üìà I‚Äôll give you copy-pasteable code you can run on your Mac to: 1. Detect whether those files are Parquet Snappy / ZSTD / uncompressed. 2. Time how long it takes to read those ~100 files. 3. Show a recommended ‚Äúbest way‚Äù to read them now and later scale to all 85k. I‚Äôll assume you‚Äôre okay installing pyarrow, duckdb, and polars via pip if they‚Äôre not there. ‚∏ª 0. Setup (once) In a venv or global, do: pip install pyarrow duckdb polars ‚∏ª 1. Check compression codec of your Parquet files Create a file, say: parquet_codec_check.py from pathlib import Path import pyarrow.parquet as pq BASE_DIR = Path("/Users/abhishek/workspace/nfo/newer data stocks/new 2025 data/nov 4 to nov 18 new stocks data/processed_output/raw_options") FILES_TEXT = """ nifty2520622050ce.parquet nifty2520622050pe.parquet nifty2520622100ce.parquet nifty2520622100pe.parquet nifty2520622150ce.parquet nifty2520622150pe.parquet nifty2520622200ce.parquet nifty2520622200pe.parquet nifty2520622250ce.parquet nifty2520622250pe.parquet nifty2520622300ce.parquet nifty2520622300pe.parquet nifty2520622350ce.parquet nifty2520622350pe.parquet nifty2520622400ce.parquet nifty2520622400pe.parquet nifty2520622450ce.parquet nifty2520622450pe.parquet nifty2520622500ce.parquet nifty2520622500pe.parquet nifty2520622550ce.parquet nifty2520622550pe.parquet nifty2520622600ce.parquet nifty2520622600pe.parquet nifty2520622650ce.parquet nifty2520622650pe.parquet nifty2520622700ce.parquet nifty2520622700pe.parquet nifty2520622750ce.parquet nifty2520622750pe.parquet nifty2520622800ce.parquet nifty2520622800pe.parquet nifty2520622850ce.parquet nifty2520622850pe.parquet nifty2520622900ce.parquet nifty2520622900pe.parquet nifty2520622950ce.parquet nifty2520622950pe.parquet nifty2520623000ce.parquet nifty2520623000pe.parquet nifty2520623050ce.parquet nifty2520623050pe.parquet nifty2520623100ce.parquet nifty2520623100pe.parquet nifty2520623150ce.parquet nifty2520623150pe.parquet nifty2520623200ce.parquet nifty2520623200pe.parquet nifty2520623250ce.parquet nifty2520623250pe.parquet nifty2520623300ce.parquet nifty2520623300pe.parquet nifty2520623350ce.parquet nifty2520623350pe.parquet nifty2520623400ce.parquet nifty2520623400pe.parquet nifty2520623450ce.parquet nifty2520623450pe.parquet nifty2520623500ce.parquet nifty2520623500pe.parquet nifty2520623550ce.parquet nifty2520623550pe.parquet nifty2520623600ce.parquet nifty2520623600pe.parquet nifty2520623650ce.parquet nifty2520623650pe.parquet nifty2520623700ce.parquet nifty2520623700pe.parquet nifty2520623750ce.parquet nifty2520623750pe.parquet nifty2520623800ce.parquet nifty2520623800pe.parquet nifty2520623850ce.parquet nifty2520623850pe.parquet nifty2520623900ce.parquet nifty2520623900pe.parquet nifty2520623950ce.parquet nifty2520623950pe.parquet nifty2520624000ce.parquet nifty2520624000pe.parquet nifty2520624050ce.parquet nifty2520624050pe.parquet nifty2520624100ce.parquet nifty2520624100pe.parquet nifty2520624150ce.parquet nifty2520624150pe.parquet nifty2520624200ce.parquet nifty2520624200pe.parquet nifty2520624250ce.parquet nifty2520624250pe.parquet nifty2520624300ce.parquet nifty2520624300pe.parquet nifty2520624350ce.parquet nifty2520624350pe.parquet nifty2520624400ce.parquet nifty2520624400pe.parquet nifty2520624450ce.parquet nifty2520624450pe.parquet nifty2520624500ce.parquet nifty2520624500pe.parquet """ FILES = [x.strip() for x in FILES_TEXT.splitlines() if x.strip()] def get_codecs_for_file(path: Path): pf = pq.ParquetFile(path) md = pf.metadata codecs = set() for rg in range(md.num_row_groups): rg_md = md.row_group(rg) for c in range(rg_md.num_columns): col_md = rg_md.column(c) codecs.add(col_md.compression) return codecs def main(): all_codecs = set() for name in FILES: path = BASE_DIR / name if not path.exists(): print(f"[WARN] Missing: {path}") continue codecs = get_codecs_for_file(path) all_codecs |= codecs print(f"{name}: codecs = {codecs}") print("\n=== Summary across inspected files ===") print(f"Distinct codecs: {all_codecs}") print("If you see {'SNAPPY'} ‚Üí Snappy; {'ZSTD'} ‚Üí Zstd; {'UNCOMPRESSED'} ‚Üí no compression.") if __name__ == "__main__": main() Run: python parquet_codec_check.py At the end it will show something like: ‚Ä¢ Distinct codecs: {'SNAPPY'} ‚Ä¢ or {'ZSTD'} ‚Ä¢ or {'UNCOMPRESSED'} (or multiple if mixed, which is unlikely in your workflow). That answers (1). ‚∏ª 2. Time how long it takes to read these 100 files Let‚Äôs benchmark a realistic, fast path: DuckDB reading all 100 files in one go. Create parquet_read_benchmark.py: from pathlib import Path import time import duckdb BASE_DIR = Path("/Users/abhishek/workspace/nfo/newer data stocks/new 2025 data/nov 4 to nov 18 new stocks data/processed_output/raw_options") FILES_TEXT = """ nifty2520622050ce.parquet nifty2520622050pe.parquet nifty2520622100ce.parquet nifty2520622100pe.parquet nifty2520622150ce.parquet nifty2520622150pe.parquet nifty2520622200ce.parquet nifty2520622200pe.parquet nifty2520622250ce.parquet nifty2520622250pe.parquet nifty2520622300ce.parquet nifty2520622300pe.parquet nifty2520622350ce.parquet nifty2520622350pe.parquet nifty2520622400ce.parquet nifty2520622400pe.parquet nifty2520622450ce.parquet nifty2520622450pe.parquet nifty2520622500ce.parquet nifty2520622500pe.parquet nifty2520622550ce.parquet nifty2520622550pe.parquet nifty2520622600ce.parquet nifty2520622600pe.parquet nifty2520622650ce.parquet nifty2520622650pe.parquet nifty2520622700ce.parquet nifty2520622700pe.parquet nifty2520622750ce.parquet nifty2520622750pe.parquet nifty2520622800ce.parquet nifty2520622800pe.parquet nifty2520622850ce.parquet nifty2520622850pe.parquet nifty2520622900ce.parquet nifty2520622900pe.parquet nifty2520622950ce.parquet nifty2520622950pe.parquet nifty2520623000ce.parquet nifty2520623000pe.parquet nifty2520623050ce.parquet nifty2520623050pe.parquet nifty2520623100ce.parquet nifty2520623100pe.parquet nifty2520623150ce.parquet nifty2520623150pe.parquet nifty2520623200ce.parquet nifty2520623200pe.parquet nifty2520623250ce.parquet nifty2520623250pe.parquet nifty2520623300ce.parquet nifty2520623300pe.parquet nifty2520623350ce.parquet nifty2520623350pe.parquet nifty2520623400ce.parquet nifty2520623400pe.parquet nifty2520623450ce.parquet nifty2520623450pe.parquet nifty2520623500ce.parquet nifty2520623500pe.parquet nifty2520623550ce.parquet nifty2520623550pe.parquet nifty2520623600ce.parquet nifty2520623600pe.parquet nifty2520623650ce.parquet nifty2520623650pe.parquet nifty2520623700ce.parquet nifty2520623700pe.parquet nifty2520623750ce.parquet nifty2520623750pe.parquet nifty2520623800ce.parquet nifty2520623800pe.parquet nifty2520623850ce.parquet nifty2520623850pe.parquet nifty2520623900ce.parquet nifty2520623900pe.parquet nifty2520623950ce.parquet nifty2520623950pe.parquet nifty2520624000ce.parquet nifty2520624000pe.parquet nifty2520624050ce.parquet nifty2520624050pe.parquet nifty2520624100ce.parquet nifty2520624100pe.parquet nifty2520624150ce.parquet nifty2520624150pe.parquet nifty2520624200ce.parquet nifty2520624200pe.parquet nifty2520624250ce.parquet nifty2520624250pe.parquet nifty2520624300ce.parquet nifty2520624300pe.parquet nifty2520624350ce.parquet nifty2520624350pe.parquet nifty2520624400ce.parquet nifty2520624400pe.parquet nifty2520624450ce.parquet nifty2520624450pe.parquet nifty2520624500ce.parquet nifty2520624500pe.parquet """ FILES = [x.strip() for x in FILES_TEXT.splitlines() if x.strip()] def main(): paths = [] for name in FILES: path = BASE_DIR / name if not path.exists(): print(f"[WARN] Missing: {path}") else: paths.append(str(path)) if not paths: print("No existing files found, check path/names.") return print(f"Found {len(paths)} files. Starting read benchmark with DuckDB...") con = duckdb.connect() # Build a DuckDB "read_parquet" across all files # Use a union over explicit file list to avoid glob mistakes file_list_sql = ", ".join(f"'{p}'" for p in paths) query = f""" SELECT * FROM read_parquet([{file_list_sql}]) """ t0 = time.perf_counter() df = con.execute(query).fetch_df() t1 = time.perf_counter() n_rows = len(df) elapsed = t1 - t0 rps = n_rows / elapsed if elapsed > 0 else float("inf") print(f"Loaded {len(paths)} files") print(f"Total rows: {n_rows:,}") print(f"Elapsed: {elapsed:.3f} s") print(f"Throughput: {rps:,.0f} rows/s") if __name__ == "__main__": main() Run: python parquet_read_benchmark.py You‚Äôll get: ‚Ä¢ number of files found ‚Ä¢ total rows across these 100 ‚Ä¢ time taken ‚Ä¢ rows per second That answers (2). ‚∏ª 3. Best way to read after optimizations (and later scale to 85k) For your use case (lots of relatively small Parquet files, 2‚Äì5 MB each; 85k total): Recommended pattern ‚Ä¢ Use DuckDB (or Polars lazy) to: ‚Ä¢ Read many files in a single query ‚Ä¢ Push filters down (by symbol, strike, date) ‚Ä¢ Avoid Python-level loops over 85k files A scalable pattern with glob for later: import duckdb from pathlib import Path BASE_DIR = Path("/Users/abhishek/workspace/nfo/newer data stocks/new 2025 data/nov 4 to nov 18 new stocks data/processed_output/raw_options") con = duckdb.connect() query = f""" SELECT * FROM read_parquet('{BASE_DIR}/*.parquet') WHERE symbol = 'NIFTY' AND expiry = DATE '2025-06-26' AND timestamp BETWEEN TIMESTAMP '2025-06-20 09:15:00' AND TIMESTAMP '2025-06-20 15:30:00' """ df = con.execute(query).fetch_df() This will: ‚Ä¢ Let DuckDB handle all 85k files internally, scanning only what‚Äôs needed. ‚Ä¢ Use your M3 Max CPU cores efficiently. ‚Ä¢ Keep Python overhead very low. You can later: ‚Ä¢ Replace SELECT * with only the columns you need for the backtest (saves bandwidth). ‚Ä¢ Add conditions for strikes/CE/PE etc. ‚∏ª TL;DR for future ‚Äúfull run on 85k files‚Äù 1. Keep Parquet, but use ZSTD + good encoding (we can do that after we know your current codec). 2. Use DuckDB read_parquet with globs to slice & load relevant data. 3. Run vectorized backtest logic on the resulting DataFrames (Pandas / Polars). ‚∏ª







I want to do some testing for benchmarking and spped testing for reading these files and stuff as later i want to do backtesting on these 85k files, instead of just 100 now, and i want you to do in this folder :



Nice, this is a fun little benchmark mission üß™üìà

I‚Äôll give you copy-pasteable code you can run on your Mac to:

1. Detect whether those files are Parquet Snappy / ZSTD / uncompressed.

2. Time how long it takes to read those ~100 files.

3. Show a recommended ‚Äúbest way‚Äù to read them now and later scale to all 85k.

I‚Äôll assume you‚Äôre okay installing pyarrow, duckdb, and polars via pip if they‚Äôre not there.

‚∏ª

Setup (once)

In a venv or global, do:

pip install pyarrow duckdb polars

‚∏ª

Check compression codec of your Parquet files

Create a file, say:

parquet_codec_check.py

from pathlib import Path

import pyarrow.parquet as pq

BASE_DIR = Path("/Users/abhishek/workspace/nfo/newer data stocks/new 2025 data/nov 4 to nov 18 new stocks data/processed_output/raw_options")

FILES_TEXT = """

nifty2520622050ce.parquet

nifty2520622050pe.parquet

nifty2520622100ce.parquet

nifty2520622100pe.parquet

nifty2520622150ce.parquet

nifty2520622150pe.parquet

nifty2520622200ce.parquet

nifty2520622200pe.parquet

nifty2520622250ce.parquet

nifty2520622250pe.parquet

nifty2520622300ce.parquet

nifty2520622300pe.parquet

nifty2520622350ce.parquet

nifty2520622350pe.parquet

nifty2520622400ce.parquet

nifty2520622400pe.parquet

nifty2520622450ce.parquet

nifty2520622450pe.parquet

nifty2520622500ce.parquet

nifty2520622500pe.parquet

nifty2520622550ce.parquet

nifty2520622550pe.parquet

nifty2520622600ce.parquet

nifty2520622600pe.parquet

nifty2520622650ce.parquet

nifty2520622650pe.parquet

nifty2520622700ce.parquet

nifty2520622700pe.parquet

nifty2520622750ce.parquet

nifty2520622750pe.parquet

nifty2520622800ce.parquet

nifty2520622800pe.parquet

nifty2520622850ce.parquet

nifty2520622850pe.parquet

nifty2520622900ce.parquet

nifty2520622900pe.parquet

nifty2520622950ce.parquet

nifty2520622950pe.parquet

nifty2520623000ce.parquet

nifty2520623000pe.parquet

nifty2520623050ce.parquet

nifty2520623050pe.parquet

nifty2520623100ce.parquet

nifty2520623100pe.parquet

nifty2520623150ce.parquet

nifty2520623150pe.parquet

nifty2520623200ce.parquet

nifty2520623200pe.parquet

nifty2520623250ce.parquet

nifty2520623250pe.parquet

nifty2520623300ce.parquet

nifty2520623300pe.parquet

nifty2520623350ce.parquet

nifty2520623350pe.parquet

nifty2520623400ce.parquet

nifty2520623400pe.parquet

nifty2520623450ce.parquet

nifty2520623450pe.parquet

nifty2520623500ce.parquet

nifty2520623500pe.parquet

nifty2520623550ce.parquet

nifty2520623550pe.parquet

nifty2520623600ce.parquet

nifty2520623600pe.parquet

nifty2520623650ce.parquet

nifty2520623650pe.parquet

nifty2520623700ce.parquet

nifty2520623700pe.parquet

nifty2520623750ce.parquet

nifty2520623750pe.parquet

nifty2520623800ce.parquet

nifty2520623800pe.parquet

nifty2520623850ce.parquet

nifty2520623850pe.parquet

nifty2520623900ce.parquet

nifty2520623900pe.parquet

nifty2520623950ce.parquet

nifty2520623950pe.parquet

nifty2520624000ce.parquet

nifty2520624000pe.parquet

nifty2520624050ce.parquet

nifty2520624050pe.parquet

nifty2520624100ce.parquet

nifty2520624100pe.parquet

nifty2520624150ce.parquet

nifty2520624150pe.parquet

nifty2520624200ce.parquet

nifty2520624200pe.parquet

nifty2520624250ce.parquet

nifty2520624250pe.parquet

nifty2520624300ce.parquet

nifty2520624300pe.parquet

nifty2520624350ce.parquet

nifty2520624350pe.parquet

nifty2520624400ce.parquet

nifty2520624400pe.parquet

nifty2520624450ce.parquet

nifty2520624450pe.parquet

nifty2520624500ce.parquet

nifty2520624500pe.parquet

"""

FILES = [x.strip() for x in FILES_TEXT.splitlines() if x.strip()]

def get_codecs_for_file(path: Path):

pf = pq.ParquetFile(path)

md = pf.metadata

codecs = set()for rg in range(md.num_row_groups):

rg_md = md.row_group(rg)

for c in range(rg_md.num_columns):

col_md = rg_md.column(c)

codecs.add(col_md.compression)return codecs

def main():

all_codecs = set()

for name in FILES:

path = BASE_DIR / name

if not path.exists():

print(f"[WARN] Missing: {path}")

continue



codecs = get_codecs_for_file(path)

all_codecs |= codecs

print(f"{name}: codecs = {codecs}")print("\n=== Summary across inspected files ===")print(f"Distinct codecs: {all_codecs}")print("If you see {'SNAPPY'} ‚Üí Snappy; {'ZSTD'} ‚Üí Zstd; {'UNCOMPRESSED'} ‚Üí no compression.")

if name == "main":

main()

Run:

python parquet_codec_check.py

At the end it will show something like:

‚Ä¢ Distinct codecs: {'SNAPPY'}

‚Ä¢ or {'ZSTD'}

‚Ä¢ or {'UNCOMPRESSED'} (or multiple if mixed, which is unlikely in your workflow).

That answers (1).

‚∏ª

Time how long it takes to read these 100 files

Let‚Äôs benchmark a realistic, fast path: DuckDB reading all 100 files in one go.

Create parquet_read_benchmark.py:

from pathlib import Path

import time

import duckdb

BASE_DIR = Path("/Users/abhishek/workspace/nfo/newer data stocks/new 2025 data/nov 4 to nov 18 new stocks data/processed_output/raw_options")

FILES_TEXT = """

nifty2520622050ce.parquet

nifty2520622050pe.parquet

nifty2520622100ce.parquet

nifty2520622100pe.parquet

nifty2520622150ce.parquet

nifty2520622150pe.parquet

nifty2520622200ce.parquet

nifty2520622200pe.parquet

nifty2520622250ce.parquet

nifty2520622250pe.parquet

nifty2520622300ce.parquet

nifty2520622300pe.parquet

nifty2520622350ce.parquet

nifty2520622350pe.parquet

nifty2520622400ce.parquet

nifty2520622400pe.parquet

nifty2520622450ce.parquet

nifty2520622450pe.parquet

nifty2520622500ce.parquet

nifty2520622500pe.parquet

nifty2520622550ce.parquet

nifty2520622550pe.parquet

nifty2520622600ce.parquet

nifty2520622600pe.parquet

nifty2520622650ce.parquet

nifty2520622650pe.parquet

nifty2520622700ce.parquet

nifty2520622700pe.parquet

nifty2520622750ce.parquet

nifty2520622750pe.parquet

nifty2520622800ce.parquet

nifty2520622800pe.parquet

nifty2520622850ce.parquet

nifty2520622850pe.parquet

nifty2520622900ce.parquet

nifty2520622900pe.parquet

nifty2520622950ce.parquet

nifty2520622950pe.parquet

nifty2520623000ce.parquet

nifty2520623000pe.parquet

nifty2520623050ce.parquet

nifty2520623050pe.parquet

nifty2520623100ce.parquet

nifty2520623100pe.parquet

nifty2520623150ce.parquet

nifty2520623150pe.parquet

nifty2520623200ce.parquet

nifty2520623200pe.parquet

nifty2520623250ce.parquet

nifty2520623250pe.parquet

nifty2520623300ce.parquet

nifty2520623300pe.parquet

nifty2520623350ce.parquet

nifty2520623350pe.parquet

nifty2520623400ce.parquet

nifty2520623400pe.parquet

nifty2520623450ce.parquet

nifty2520623450pe.parquet

nifty2520623500ce.parquet

nifty2520623500pe.parquet

nifty2520623550ce.parquet

nifty2520623550pe.parquet

nifty2520623600ce.parquet

nifty2520623600pe.parquet

nifty2520623650ce.parquet

nifty2520623650pe.parquet

nifty2520623700ce.parquet

nifty2520623700pe.parquet

nifty2520623750ce.parquet

nifty2520623750pe.parquet

nifty2520623800ce.parquet

nifty2520623800pe.parquet

nifty2520623850ce.parquet

nifty2520623850pe.parquet

nifty2520623900ce.parquet

nifty2520623900pe.parquet

nifty2520623950ce.parquet

nifty2520623950pe.parquet

nifty2520624000ce.parquet

nifty2520624000pe.parquet

nifty2520624050ce.parquet

nifty2520624050pe.parquet

nifty2520624100ce.parquet

nifty2520624100pe.parquet

nifty2520624150ce.parquet

nifty2520624150pe.parquet

nifty2520624200ce.parquet

nifty2520624200pe.parquet

nifty2520624250ce.parquet

nifty2520624250pe.parquet

nifty2520624300ce.parquet

nifty2520624300pe.parquet

nifty2520624350ce.parquet

nifty2520624350pe.parquet

nifty2520624400ce.parquet

nifty2520624400pe.parquet

nifty2520624450ce.parquet

nifty2520624450pe.parquet

nifty2520624500ce.parquet

nifty2520624500pe.parquet

"""

FILES = [x.strip() for x in FILES_TEXT.splitlines() if x.strip()]

def main():

paths = []

for name in FILES:

path = BASE_DIR / name

if not path.exists():

print(f"[WARN] Missing: {path}")

else:

paths.append(str(path))

if not paths:

print("No existing files found, check path/names.")

returnprint(f"Found {len(paths)} files. Starting read benchmark with DuckDB...")



con = duckdb.connect()# Build a DuckDB "read_parquet" across all files# Use a union over explicit file list to avoid glob mistakes

file_list_sql = ", ".join(f"'{p}'" for p in paths)



query = f"""

SELECT *

FROM read_parquet([{file_list_sql}])

"""



t0 = time.perf_counter()

df = con.execute(query).fetch_df()

t1 = time.perf_counter()



n_rows = len(df)

elapsed = t1 - t0

rps = n_rows / elapsed if elapsed > 0 else float("inf")print(f"Loaded {len(paths)} files")print(f"Total rows: {n_rows:,}")print(f"Elapsed: {elapsed:.3f} s")print(f"Throughput: {rps:,.0f} rows/s")

if name == "main":

main()

Run:

python parquet_read_benchmark.py

You‚Äôll get:

‚Ä¢ number of files found

‚Ä¢ total rows across these 100

‚Ä¢ time taken

‚Ä¢ rows per second

That answers (2).

‚∏ª

Best way to read after optimizations (and later scale to 85k)

For your use case (lots of relatively small Parquet files, 2‚Äì5 MB each; 85k total):

Recommended pattern

‚Ä¢ Use DuckDB (or Polars lazy) to:

‚Ä¢ Read many files in a single query

‚Ä¢ Push filters down (by symbol, strike, date)

‚Ä¢ Avoid Python-level loops over 85k files

A scalable pattern with glob for later:

import duckdb

from pathlib import Path

BASE_DIR = Path("/Users/abhishek/workspace/nfo/newer data stocks/new 2025 data/nov 4 to nov 18 new stocks data/processed_output/raw_options")

con = duckdb.connect()

query = f"""

SELECT *

FROM read_parquet('{BASE_DIR}/*.parquet')

WHERE symbol = 'NIFTY'

AND expiry = DATE '2025-06-26'

AND timestamp BETWEEN TIMESTAMP '2025-06-20 09:15:00'

AND TIMESTAMP '2025-06-20 15:30:00'

"""

df = con.execute(query).fetch_df()

This will:

‚Ä¢ Let DuckDB handle all 85k files internally, scanning only what‚Äôs needed.

‚Ä¢ Use your M3 Max CPU cores efficiently.

‚Ä¢ Keep Python overhead very low.

You can later:

‚Ä¢ Replace SELECT * with only the columns you need for the backtest (saves bandwidth).

‚Ä¢ Add conditions for strikes/CE/PE etc.

‚∏ª

TL;DR for future ‚Äúfull run on 85k files‚Äù

1. Keep Parquet, but use ZSTD + good encoding (we can do that after we know your current codec).

2. Use DuckDB read_parquet with globs to slice & load relevant data.

3. Run vectorized backtest logic on the resulting DataFrames (Pandas / Polars).

‚∏ª



Added a single helper script to benchmark and inspect your option parquet files.



parquet_benchmark.py: samples files from the current folder (default 100), optionally shuffles, checks compression codecs with pyarrow, and times a DuckDB scan either via explicit file list or a glob (good for the full ~85k). Uses Arrow fetch to avoid Pandas overhead; supports projecting specific columns with --columns.

Run it from /Users/abhishek/workspace/nfo/data/raw/options (install deps first: pip install duckdb pyarrow):

Codec check on 100 files: python parquet_benchmark.py --codec-check

Larger sample benchmark: python parquet_benchmark.py --sample 500

Full-folder DuckDB glob scan (ignores sample): python parquet_benchmark.py --use-glob

Column-pruned glob run: python parquet_benchmark.py --use-glob --columns symbol,expiry,strike

Next steps: install deps if needed, then run the codec check and a sample benchmark to size the workload before scaling to the full set.

1 file changed

Undo



View all changes



parquet_benchmark.py

+161

-0





I wanted you to run and analyse result, can you do that, if not i will



Ran the benchmark with schema-union enabled to handle mixed files.



Command: python parquet_benchmark.py --codec-check --sample 100

Codecs across the sampled files: {'ZSTD'} (Snappy/none not seen).

DuckDB read (explicit list, union_by_name=True): 6,830,492 rows in 10.892 s ‚Üí ~627,106 rows/s.

Notes:



The script now defaults to union_by_name to avoid schema mismatch errors; use --strict-schema if you want it off.

For the full set, use the glob path: python parquet_benchmark.py --use-glob --columns symbol,expiry,strike,timestamp,price (narrow columns to speed up).

File: parquet_benchmark.py updated in /Users/abhishek/workspace/nfo/data/raw/options.

1 file changed

Undo



View all changes



parquet_benchmark.py

+10

-3



Okay, is this the fastest reading speed - can you try other approaches if possible, find the fastest way to read those for backtesting please, if needed you can load into ram or anything - considering we want to backtest on 85k of these files i need the fastest approach -- dont change anything in data though for now, if you need to create - create copies and then alter as needed.



Ran multiple read paths on the same 100-file sample (union-by-name) to see what‚Äôs fastest for backtesting.

DuckDB full columns: 6,830,492 rows in 11.709 s ‚Üí ~0.58 M rows/s.

DuckDB projected (timestamp, price, qty, symbol, opt_type, strike): 6,830,492 rows in 1.007 s ‚Üí ~6.78 M rows/s.

PyArrow dataset projected (same cols): 6,830,492 rows in 0.380 s ‚Üí ~18.0 M rows/s.

Polars scan via PyArrow dataset (same cols): 6,830,492 rows in 0.408 s ‚Üí ~16.7 M rows/s.

Codecs remain {'ZSTD'} across the sampled files. Arrow emitted harmless sysctl warnings in the sandbox.

Fastest approach (keep data unchanged):

from pathlib import Pathimport pyarrow.dataset as ds



base = Path("/Users/abhishek/workspace/nfo/data/raw/options")

cols = ["timestamp", "price", "qty", "symbol", "opt_type", "strike"]



dataset = ds.dataset(base, format="parquet") # Arrow will union schemas

table = dataset.to_table(columns=cols, use_threads=True)# `table` is an Arrow table; convert if needed:# import polars as pl; df = pl.from_arrow(table)

For SQL-like slicing with DuckDB but much faster than ‚ÄúSELECT *‚Äù:

import duckdbfrom pathlib import Path



base = Path("/Users/abhishek/workspace/nfo/data/raw/options")

cols = ["timestamp", "price", "qty", "symbol", "opt_type", "strike"]

con = duckdb.connect()

df = con.execute(

f"SELECT {', '.join(cols)} "

f"FROM read_parquet('{base}/*.parquet', union_by_name=True)"

).fetch_arrow_table() # or .fetch_df()

Recommendations for the 85k run:



Use projection (only needed columns) and push filters (date/symbol/strike) to avoid scanning everything.

Prefer PyArrow dataset or Polars scan_pyarrow_dataset for fastest raw read; use DuckDB if you need SQL joins/aggregations, still with column projection and union_by_name=true.

If you need a pandas DataFrame downstream, consider staying in Arrow/Polars or DuckDB as long as possible to avoid Python-level overhead.

Want me to script these options into parquet_benchmark.py so you can rerun them with a flag?

yes, maybe create a new script, keeping this one intact, create one : parquet_benchmark_2.py

Also, explain me in plain language what happened :

DuckDB full columns: 6,830,492 rows in 11.709 s ‚Üí ~0.58 M rows/s.

DuckDB projected (timestamp, price, qty, symbol, opt_type, strike): 6,830,492 rows in 1.007 s ‚Üí ~6.78 M rows/s.

PyArrow dataset projected (same cols): 6,830,492 rows in 0.380 s ‚Üí ~18.0 M rows/s.

Polars scan via PyArrow dataset (same cols): 6,830,492 rows in 0.408 s ‚Üí ~16.7 M rows/s.

-- like what is projected here, did you run on whole thing or not or what -- please explain to me - how pyarrow works, how polars work, and more stuff as well -- why they performed better than earlier one that took 10.9 secs -- be description, and also this time make sure to read the whole damn thing not just extrapolation



Added a new benchmark script and explained the results plainly.

What happened in the earlier run (all on the same 100-file sample = 6,830,492 rows):



DuckDB full columns: 11.709 s ‚Üí ~0.58M rows/s. Reads every column from each file; more data to decompress/transfer.

DuckDB projected (timestamp, price, qty, symbol, opt_type, strike): 1.007 s ‚Üí ~6.78M rows/s. Same files/rows, but only those 6 columns are read, so far less data and faster.

PyArrow dataset projected: 0.380 s ‚Üí ~18.0M rows/s. Same files/rows, only projected columns; Arrow‚Äôs dataset scanner is a very fast C++ reader using multithreading and no SQL planner overhead.

Polars scan via PyArrow dataset projected: 0.408 s ‚Üí ~16.7M rows/s. Same projection; Polars lazy wraps the Arrow dataset scanner and then materializes, adding a small overhead vs pure Arrow but still very fast.

The ‚Äúprojection‚Äù just means ‚Äúread only these columns.‚Äù All benchmarks read the same 6.83M rows from the same 100 files; we did not extrapolate to the full 85k yet. No data was modified.

Why PyArrow/Polars beat DuckDB full:



Less data: projecting 6 columns vs all columns slashes IO/decompression.

Arrow dataset uses a tight C++ scan and multithreading, with minimal overhead.

DuckDB full had to read and materialize every column, so more bytes to read/decompress/transfer.

DuckDB projected got much faster once columns were pruned; still a bit slower than Arrow due to SQL planning and result materialization overhead.

New script:



Added parquet_benchmark_2.py alongside the original. It benchmarks:DuckDB full scan

DuckDB projected columns

PyArrow dataset projected

Polars lazy scan projected

Defaults: 100-file sample (from current directory), columns timestamp,price,qty,symbol,opt_type,strike, union_by_name for schema differences.

Usage examples (run from /Users/abhishek/workspace/nfo/data/raw/options):python parquet_benchmark_2.py

python parquet_benchmark_2.py --sample 500 --shuffle

python parquet_benchmark_2.py --columns timestamp,price,qty

File: parquet_benchmark_2.py (kept the original parquet_benchmark.py intact).

How PyArrow works (in this context):



pyarrow.dataset.dataset([...], format="parquet") builds a dataset over many parquet files and unifies schema by name.

to_table(columns=cols, use_threads=True) performs a C++ scan, reading only those columns, multithreaded by default.

How Polars works here:



Uses scan_pyarrow_dataset(dataset) to get a lazy plan over the same Arrow dataset, applies select(cols), then collect() to execute. It reuses Arrow‚Äôs fast scan and then builds a Polars DataFrame.

For the full 85k:



Stick to projection and filters (date/symbol/strike) to avoid scanning all data.

Fastest raw read: Arrow dataset (or Polars scan over it). If you need SQL/joins: DuckDB with union_by_name=true and projection plus filters in the query.

Avoid reading all columns unless necessary.

1 file changed

Undo



View all changes



parquet_benchmark_2.py

+160

-0





For these 100 files, i want to read whole data - all columns while we are testing



Ran a full-column read on the same 100 files (all columns, union_by_name):



Command: python parquet_benchmark.py --sample 100

Rows: 6,830,492

Elapsed: 12.288 s

Throughput: ~555,888 rows/s

This is the full-data path (no column projection). Let me know if you want the same measurement via the new parquet_benchmark_2.py (DuckDB full) or with a different sample size.

And i need you to run that script for me, and then come back to me with results -- for all approaches, no projections - full data read - all approaches, then we find the fastest approaches, so run it now and then suggest me more improvements



Full-data (all columns) benchmark on the same 100 files (6,830,492 rows), union-by-name enabled:



DuckDB full: 11.776 s ‚Üí ~580,043 rows/s

PyArrow dataset full: 5.861 s ‚Üí ~1,165,381 rows/s

Polars scan full: 9.120 s ‚Üí ~748,928 rows/s

(Arrow emitted harmless sysctl warnings due to sandbox restrictions.)

Fastest full read here: PyArrow dataset. DuckDB full is ~2x slower, Polars full ~1.6x slower than Arrow.

How to rerun the full read test:

python - <<'PY'

import time, itertools

from pathlib import Path

import duckdb, pyarrow.dataset as ds, polars as pl



files = [str(p) for p in itertools.islice(Path('.').glob('*.parquet'), 100)]print(f"Benchmarking full columns on {len(files)} files")



def bench(name, fn):

t0 = time.perf_counter(); rows = fn(); dt = time.perf_counter()-t0

print(f"{name:<26} {rows:>12,} rows {dt:6.3f} s {rows/dt:>10,.0f} rows/s")



bench("duckdb full", lambda: duckdb.connect()

.execute(f"SELECT * FROM read_parquet([{', '.join(repr(f) for f in files)}], union_by_name=True)")

.fetch_arrow_table().num_rows)



ds_files = ds.dataset(files, format="parquet")

bench("pyarrow dataset full", lambda: ds_files.to_table(use_threads=True).num_rows)



bench("polars scan full", lambda: pl.scan_pyarrow_dataset(ds_files).collect().height)

PY

Notes and suggested improvements (for the 85k run):



Stick with PyArrow dataset for fastest full-table materialization; it‚Äôs the current winner.

If you can avoid full-column scans in the real backtest, apply projection and filters (date/symbol/strike) ‚Äî you‚Äôve seen this cuts times by >10x.

For DuckDB runs, keep union_by_name=true; optionally set threads: PRAGMA threads=<core_count>;.

For string-heavy data, consider dictionary encoding or ZSTD tuning later (requires rewriting files; you asked not to change data now).

Files:



New benchmark script already added: parquet_benchmark_2.py (unchanged for this run).

Okay, now i want you to do another test - with new script - where there is real calculations happening, so assume this strategy buy when ema 5 is greater than ema 21 on a tick level - each tick is one secs and then write the result for each file in a csv - so result is one csv with profit and loss, for all the 100 files, and then measure involved time in all approaches



To do this right I need a couple of details about the strategy/output so I don‚Äôt bake in the wrong assumptions:

Which price column should we trade on? The files have a price column (string) plus many others; should I use price converted to float?

Position logic: long when EMA5 > EMA21, flat otherwise, or also short when EMA5 < EMA21?

Entry/exit: assume one unit, mark-to-market each tick; start flat; close position at the last tick?

Costs/slippage: ignore, or apply a per-trade fee/spread?

Output CSV: one row per file with total P&L (and maybe number of trades), saved in this folder (e.g., strategy_results.csv)?

Once you confirm, I‚Äôll implement the benchmark script (DuckDB/PyArrow/Polars variants), run it on the 100 files, and report timings and P&L.

yes, use price.

long when ema5 is greater than ema 21, no shorting

close when ema21>ema5 or last tick of file

ignore slippage for now

yes one row per file

Now, run full on all variants and measure time, and also make sure that results are 100% matching, not mixing is happening in any



Ran the EMA5/EMA21 long-only strategy over the first 100 parquet files using the new strategy_benchmark.py. Strategy recap: enter long when EMA5 > EMA21, exit when EMA21 >= EMA5 or at the final tick, 1-unit size, no shorts, no costs. P&L is per file. Four files lacked a price column and were skipped in all approaches to keep results aligned.

Timings (same 6,830,484 rows processed across the remaining 96 files; union_by_name enabled):



DuckDB read ‚Üí Python EMA: 1.651 s (‚âà 4.14 M rows/s)

PyArrow dataset ‚Üí Python EMA: 1.023 s (‚âà 6.68 M rows/s)

Polars read ‚Üí Python EMA: 0.955 s (‚âà 7.15 M rows/s)

All approaches produced identical P&L per file on the processed set.

Artifacts:



CSV with per-file P&L (from the PyArrow run): strategy_results.csv in /Users/abhishek/workspace/nfo/data/raw/options.

Script: strategy_benchmark.py (keeps the other benchmarks intact).

Notes/next improvements:



Some files (banknifty25jul54600pe.parquet, banknifty25jan43100pe.parquet, banknifty25may53000pe.parquet, banknifty25jul63500ce.parquet) have no price; decide whether to drop or fix those before scaling.

For the full 85k, PyArrow/Polars are currently fastest; you can parallelize per-file processing (e.g., concurrent.futures.ThreadPoolExecutor since IO-bound) to cut wall time further.

If you need DuckDB end-to-end (for joins/filters), stick to column projection and union_by_name=true; but for pure scan + custom Python logic, Arrow/Polars remain quickest.

1 file changed

Undo



View all changes



strategy_benchmark.py

+260

-0





Quick question, why this one came out take less time than just reading or you just did calculation times, as earlier read time was :

Full-data (all columns) benchmark on the same 100 files (6,830,492 rows), union-by-name enabled:

DuckDB full: 11.776 s ‚Üí ~580,043 rows/s

PyArrow dataset full: 5.861 s ‚Üí ~1,165,381 rows/s

Polars scan full: 9.120 s ‚Üí ~748,928 rows/s

And now, you shared :

DuckDB read ‚Üí Python EMA: 1.651 s (‚âà 4.14 M rows/s)

PyArrow dataset ‚Üí Python EMA: 1.023 s (‚âà 6.68 M rows/s)

Polars read ‚Üí Python EMA: 0.955 s (‚âà 7.15 M rows/s)



Those faster times are because the strategy run was only reading two columns (timestamp, price) per file, not every column. In the earlier ‚Äúfull-data‚Äù benchmark we pulled all columns from all 100 files, which means far more bytes to read/decompress (lots of string columns). In the strategy benchmark:

We select only timestamp and price, so the IO/decompression volume is much smaller.

Four files without price were skipped consistently across all paths, leaving 96 files (6,830,484 rows vs 6,830,492 before‚Äînegligible row difference but slightly less data).

The EMA math itself is tiny overhead relative to the read; the speedup is from column projection and less schema/decoding work.

So the numbers you‚Äôre seeing (DuckDB ~1.65s, Arrow ~1.02s, Polars ~0.96s) are essentially ‚Äúread + light compute‚Äù on just two columns, versus the earlier 11.8s/5.9s/9.1s when ingesting every column.

Now, do one where you have to consider order book as well and volume as well, update strategy to include those someway as well, and also write overall volume trader and orderbook analysis in csv, and then tell me the final results, and also give suggestion on how much time it will take for 85k files instead of 100 if i were to do it



Updated strategy with order book + volume filter and ran it on 100 files.

What changed in the strategy



Still EMA(5/21) long-only, price-based.

Only enter if: EMA5 > EMA21 AND top-of-book spread is tight ( (ask0-bid0)/mid <= 5 bps ) AND volume >= 1.

Exit on EMA5 <= EMA21 or last tick. Position size = 1. No shorts, no costs.

Per-file metrics: pnl, trades, total_volume, avg_spread, avg_bid_qty, avg_ask_qty, rows.

Run results (same 4 files skipped because they lack price; processed 96 files = 6,830,484 rows):



DuckDB: 3.700 s ‚Üí ~1.85 M rows/s

PyArrow dataset: 2.668 s ‚Üí ~2.56 M rows/s

Polars: 2.824 s ‚Üí ~2.42 M rows/s

All approaches produced matching metrics per file.

Artifacts



Script: strategy_benchmark_orderbook.py (keeps prior scripts intact).

Output CSV: strategy_results_orderbook.csv in /Users/abhishek/workspace/nfo/data/raw/options with per-file pnl/trades/volume/order-book stats.

Why slower than the price-only strategy



We pulled more columns (volume + top-of-book fields) and did extra calculations (spread filter, aggregates), so more data and compute.

Estimate for 85k files



Roughly 850x the 100-file run. Using the fastest path (PyArrow ~2.668 s for 6.83M rows):Wall-clock naive linear estimate: ~2.668 s * 850 ‚âà 2,268 s ‚âà 38 minutes on a single process.

You can cut this by parallelizing across files (I/O bound) with a thread pool or process pool; with, say, 8‚Äì12 workers on your machine, expect something like 3‚Äì6 minutes if storage keeps up.

DuckDB/Polars will scale similarly; Arrow remains the fastest scan in these tests.

Next tweaks if you want more speed/robustness



Parallelize per-file processing (threads are fine here; GIL is mostly released in Arrow).

Consider a schema check to drop files missing price up-front or add a default; right now we skip them.

If you can reduce columns further (e.g., only bp0/sp0/bq0/sq0 instead of deeper ladder), that saves IO.

If you need to tolerate wider spreads/volume rules, adjust the 5 bps and vol>=1 thresholds in strategy_benchmark_orderbook.py.

Let me know if you want me to parallelize the run or change the order-book/volume thresholds and rerun.

1 file changed

Undo



View all changes



strategy_benchmark_orderbook.py

+337

-0





Parallelize per-file processing and then try to get fast processing, use best approaches



Added parallel processing for per-file work in strategy_benchmark_orderbook.py and reran on 100 files.

Current run (order book + volume strategy, same 4 files skipped due to missing price; 96 files, 6,830,484 rows):

DuckDB: 3.785 s ‚Üí ~1.80 M rows/s

PyArrow dataset: 2.704 s ‚Üí ~2.53 M rows/s

Polars: 2.770 s ‚Üí ~2.47 M rows/s

All approaches matched per-file metrics; CSV saved to strategy_results_orderbook.csv.

Parallelization detail:



Per-file processing now uses a thread pool (default workers capped to CPU count). Reads + compute are parallelized; global combine is unchanged.

Why not faster than the prior serial run:



The workload is still largely IO/parse bound per file; with smallish files and a fast SSD, the thread pool helps a bit but we‚Äôre limited by per-file overhead and Python EMA loops.

If you want to push speed further:



Increase workers: run python strategy_benchmark_orderbook.py --sample 100 after editing the script to set max_workers higher (e.g., 12‚Äì16) for more concurrency.

Vectorize EMA and filters in Polars or DuckDB SQL to reduce Python-loop overhead; currently EMAs are pure Python.

Filter columns aggressively (already limited to timestamp, price, volume, bp0, sp0, bq0, sq0).

85k-file estimate with current fastest path (PyArrow ~2.7s for 100 files):



Linear: ~2.7s * 850 ‚âà ~2,300s ‚âà ~38 minutes single-process. With 12 workers and good disk throughput, expect ~3‚Äì6 minutes.

Files touched:



Updated: strategy_benchmark_orderbook.py (adds threaded per-file processing).

Outputs: strategy_results_orderbook.csv (latest run).

1 file changed

Undo



View all changes



strategy_benchmark_orderbook.py

+33

-0



Do these and then run and give results :

Increase workers: run python strategy_benchmark_orderbook.py --sample 100 after editing the script to set max_workers higher (e.g., 12‚Äì16) for more concurrency.

Vectorize EMA and filters in Polars or DuckDB SQL to reduce Python-loop overhead; currently EMAs are pure Python.

Keep rest same



Parallelized run with vectorized EMAs/filters and 16 workers on the same 100-file sample (still skipping the 4 files missing price; 96 files, 6,830,484 rows):

DuckDB: 0.731 s ‚Üí ~9.34 M rows/s

PyArrow dataset: 0.712 s ‚Üí ~9.59 M rows/s

Polars: 0.625 s ‚Üí ~10.92 M rows/s

All approaches produced matching per-file metrics; output CSV refreshed: strategy_results_orderbook.csv.

Changes made



strategy_benchmark_orderbook.py now:Uses a thread pool (default workers = min(16, CPU cores); run shown with --workers 16).

Computes EMA5/EMA21 with Polars‚Äô ewm_mean (vectorized) and precomputes spread/volume flags in Polars to shrink Python-loop work (only the entry/exit state machine remains in Python).

Estimate for full 85k with this setup (fastest path ‚âà0.7 s per 100 files, linear):



~0.7 s * 850 ‚âà ~595 s ‚âà ~10 minutes single-process.

With 16 threads and good disk throughput, expect a few minutes (2‚Äì4 min) if the filesystem keeps up; real-world depends on IO bandwidth and CPU.

If you want, I can set --workers to 12/16 by default and rerun, or add a fully vectorized entry/exit to remove the remaining Python loop (would need a bit more logic).

Based on the detailed chat log you provided, ChatGPT guided you correctly through the initial phases: checking codecs, comparing libraries (DuckDB vs. PyArrow vs. Polars), realizing that column projection (reading only what you need) is the biggest speed hack, and finally adding threading.

However, for scaling to 85,000 files and actual backtesting, the current "threaded for-loop" approach will hit a ceiling.

Here are the concrete better suggestions to improve speed, scalability, and robustness.

1. The "Small File" Problem (Architecture Change)
Reading 85k separate files (2‚Äì5MB each) is inefficient. The Operating System overhead to open/close file handles and list directories will eventually become a bottleneck, regardless of how fast Python is.

Suggestion: Write a one-time script to repartition your data. Group the 85k files into larger chunks (e.g., by Month or Year).

Why: Reading 1 file of 1GB is significantly faster than reading 1,000 files of 1MB due to sequential I/O and reduced metadata overhead.

Action: Create NIFTY_2025_01.parquet, NIFTY_2025_02.parquet, etc.

2. Solve the "Python Loop" Bottleneck with Numba
ChatGPT moved you to "Vectorized EMAs" (using Polars), which is great. However, it mentioned: "only the entry/exit state machine remains in Python." In backtesting, strategy logic (If Long -> Check Exit -> Else Check Entry) is iterative and usually requires a slow Python for loop.

Suggestion: Use Numba. It compiles Python functions to machine code (LLVM) at runtime.

Impact: It turns your strategy logic from "Python speed" to "C++ speed." This is standard practice in HFT/Quant backtesting in Python.

How: Convert Polars columns to Numpy arrays ‚Üí Pass to Numba function ‚Üí Return signals.

3. Use Polars LazyFrame and scan_parquet
Instead of manually managing a ThreadPoolExecutor (which is what the last script did), use Polars' native lazy execution engine.

Suggestion: pl.scan_parquet("folder/*.parquet").

Why: Polars writes its own query plan. It knows how to parallelize reading and processing better than Python threads can. It handles memory streaming so you don't OOM (Out of Memory) when loading 85k files.

4. Code Improvement: The "Numba + Polars" Hybrid
Here is the "Best Way" code structure. It uses Polars for data loading/indicators (vectorized) and Numba for the trade logic (compiled).

Prerequisite: pip install numba

Python
import polars as pl
import numpy as np
from numba import njit
import glob
import time

# 1. The Strategy Logic (Compiled to Machine Code)
@njit  # <--- This decorator makes it run at C-speed
def run_backtest_numba(prices, ema5, ema21, ask0, bid0, vol):
    n = len(prices)
    # 0 = flat, 1 = long
    position = 0 
    pnl = 0.0
    entry_price = 0.0
    trades = 0
    
    # Pre-calculate spread bps (0.0005 = 5bps)
    # Avoid doing this inside the loop if possible, or do simple math here
    
    for i in range(1, n):
        # Calculate dynamic conditions
        spread_bps = (ask0[i] - bid0[i]) / prices[i] if prices[i] > 0 else 999
        
        if position == 0:
            # Entry Logic: EMA5 > EMA21, Tight Spread, Vol >= 1
            if (ema5[i] > ema21[i]) and (spread_bps <= 0.0005) and (vol[i] >= 1):
                position = 1
                entry_price = prices[i]
                trades += 1
                
        elif position == 1:
            # Exit Logic: EMA21 >= EMA5 (Crossunder)
            if ema21[i] >= ema5[i]:
                position = 0
                pnl += (prices[i] - entry_price)
            # Force close on last tick
            elif i == n - 1:
                pnl += (prices[i] - entry_price)
                
    return pnl, trades

def main():
    # 2. Setup Files (Using Glob to find them)
    path = "/Users/abhishek/workspace/nfo/newer data stocks/new 2025 data/nov 4 to nov 18 new stocks data/processed_output/raw_options/*.parquet"
    # Note: For 85k files, pure glob might be slow. Faster to use os.scandir if needed later.
    files = glob.glob(path)[:100] # Limit to 100 for test
    
    start_time = time.time()
    
    results = []

    # 3. Processing Loop (Can be parallelized with ProcessPoolExecutor)
    # Note: Polars usually releases GIL, but Numba + Polars is very fast even sequentially on small files.
    # For 85k files, wrap this loop in ProcessPoolExecutor.
    
    for f in files:
        try:
            # Lazy Scan -> Filter Columns -> Eager Fetch
            # We use eager load per file because files are small
            df = pl.read_parquet(
                f, 
                columns=["timestamp", "price", "volume", "ask_price_0", "bid_price_0"]
            )
            
            # Data Cleaning / Type Casting
            # Ensure price is float
            if df["price"].dtype != pl.Float64:
                df = df.with_columns(pl.col("price").cast(pl.Float64))

            # Vectorized Indicator Calculation (Polars is instant here)
            df = df.with_columns([
                pl.col("price").ewm_mean(span=5, adjust=False).alias("ema5"),
                pl.col("price").ewm_mean(span=21, adjust=False).alias("ema21"),
            ])
            
            # Drop Nulls created by indicators (if any)
            # Passing Numpy arrays to Numba
            pnl, trades = run_backtest_numba(
                df["price"].to_numpy(),
                df["ema5"].to_numpy(),
                df["ema21"].to_numpy(),
                df["ask_price_0"].to_numpy(),
                df["bid_price_0"].to_numpy(),
                df["volume"].to_numpy()
            )
            
            results.append({"file": f, "pnl": pnl, "trades": trades})

        except Exception as e:
            # Handle files with missing columns
            # print(f"Skipping {f}: {e}")
            pass

    end_time = time.time()
    print(f"Processed {len(files)} files in {end_time - start_time:.4f} seconds")
    
    # Save results
    pl.DataFrame(results).write_csv("numba_results.csv")

if __name__ == "__main__":
    main()
Why this is better:
Memory Efficient: It reads specific columns, processes, calculates PnL, and discards the dataframe immediately. It does not hold 6 million rows in memory.

No Python State Machine: The run_backtest_numba function compiles to native machine instructions. The loop that checks if position == 0 happens at processor speed, not interpreter speed.

Scalable: When you run this on 85k files, the only limit is your disk speed.

Summary of Differences
Feature	ChatGPT Last Approach	My Suggestion
Logic	Python for loop or Hybrid	Numba JIT (Compiled)
Data Reading	ThreadPoolExecutor	ProcessPool (or Polars Scan)
Speed Limit	Python Interpreter Overhead	Disk I/O Bandwidth
85k Scaling	High RAM usage / slower loops	Low RAM / Max CPU utilization
Next Step: Run the code block above on your 100 files. Compare the timing against the 0.625s record. If Numba is correctly compiled, it handles the logic pass in microseconds.

New script :



#!/usr/bin/env python3

"""

Order-book-aware EMA(5/21) strategy using Polars + Numba with process-level parallelism.



Differences vs prior scripts:

- Handles missing order-book columns by filling nulls instead of skipping files.

- Reads only the needed columns that actually exist in each file (schema-driven),

but targets full top-5 levels when available (bp/sp/bq/sq 0..4).

- Designed to be run outside the sandbox with a ProcessPoolExecutor.



Strategy:

- Go long when EMA5 > EMA21 AND spread_bps <= 5 bps AND volume >= 1.

- Exit on EMA21 >= EMA5 or last tick. No shorts, 1-unit size.



Outputs:

- Per-file metrics (pnl, trades, total_volume, avg_spread, avg_bid_qty, avg_ask_qty, rows)

written to strategy_results_numba_full.csv in the base directory.

"""



from __future__ import annotations



import argparse

import math

import os

import time

from concurrent.futures import ProcessPoolExecutor, as_completed

from dataclasses import dataclass

from pathlib import Path

from typing import Iterable, List, Optional, Sequence



import numpy as np

import polars as pl

import pyarrow.parquet as pq

from numba import njit





@njit

def run_strategy(prices, ema5, ema21, bid0, ask0, volume):

n = len(prices)

pnl = 0.0

trades = 0

pos = 0

entry = 0.0

for i in range(1, n):

spread_ok = False

if ask0[i] > 0.0 and bid0[i] > 0.0:

mid = 0.5 * (ask0[i] + bid0[i])

if mid > 0.0:

spread_bps = (ask0[i] - bid0[i]) / mid

spread_ok = spread_bps <= 0.0005

vol_ok = volume[i] >= 1.0

if pos == 0:

if ema5[i] > ema21[i] and spread_ok and vol_ok:

pos = 1

entry = prices[i]

trades += 1

else:

if ema21[i] >= ema5[i] or i == n - 1:

pnl += prices[i] - entry

pos = 0

return pnl, trades





@dataclass

class Metrics:

file: str

pnl: float

trades: int

total_volume: float

avg_spread: float

avg_bid_qty: float

avg_ask_qty: float

rows: int





def parse_args() -> argparse.Namespace:

p = argparse.ArgumentParser(description="Numba + Polars orderbook strategy (schema-aware, tolerant to missing cols)")

p.add_argument("--base-dir", type=Path, default=Path(__file__).resolve().parent, help="Directory with parquet files.")

p.add_argument("--sample", type=int, default=100, help="Number of files (<=0 means all).")

p.add_argument("--shuffle", action="store_true", help="Shuffle before sampling.")

p.add_argument("--workers", type=int, default=None, help="Process pool size (default min(16, CPU cores)).")

return p.parse_args()





def sample_files(base_dir: Path, sample: int, shuffle: bool) -> List[Path]:

iterator: Iterable[Path] = base_dir.glob("*.parquet")

files = list(iterator)

if shuffle:

import random



random.shuffle(files)

if sample > 0:

files = files[:sample]

return files





def available_columns(path: Path) -> set:

try:

schema = pq.read_schema(path)

return set(schema.names)

except Exception:

return set()





def process_one(path: Path) -> Optional[Metrics]:

# Target columns (full top-5 levels)

base_cols = ["timestamp", "price", "volume"]

book_cols = []

for lvl in range(5):

book_cols.extend(

[f"bp{lvl}", f"sp{lvl}", f"bq{lvl}", f"sq{lvl}"]

)

desired = base_cols + book_cols



avail = available_columns(path)

if "price" not in avail or "timestamp" not in avail:

return None



cols_to_read = [c for c in desired if c in avail]

if not cols_to_read:

return None



try:

df = pl.read_parquet(path, columns=cols_to_read, use_pyarrow=True)

except Exception:

return None



# Add missing desired columns as null to keep schema stable

for c in desired:

if c not in df.columns:

df = df.with_columns(pl.lit(None).alias(c))



df = (

df.with_columns(

[

pl.col("price").cast(pl.Float64),

pl.col("volume").cast(pl.Float64),

]

+ [pl.col(c).cast(pl.Float64) for c in book_cols]

)

.drop_nulls(subset=["timestamp", "price"])

.sort("timestamp")

)

if df.is_empty():

return None



df = df.with_columns(

[

(pl.col("sp0") - pl.col("bp0")).alias("spread"),

((pl.col("sp0") + pl.col("bp0")) / 2.0).alias("mid"),

pl.col("price").ewm_mean(span=5, adjust=False).alias("ema5"),

pl.col("price").ewm_mean(span=21, adjust=False).alias("ema21"),

]

)



prices = df["price"].to_numpy()

ema5 = df["ema5"].to_numpy()

ema21 = df["ema21"].to_numpy()

bid0 = df["bp0"].fill_null(0.0).to_numpy()

ask0 = df["sp0"].fill_null(0.0).to_numpy()

volume = df["volume"].fill_null(0.0).to_numpy()



if len(prices) < 2:

return None



pnl, trades = run_strategy(prices, ema5, ema21, bid0, ask0, volume)



spread_valid = df.filter(pl.col("spread").is_not_null() & pl.col("mid").is_not_null())

avg_spread = float(spread_valid["spread"].mean()) if spread_valid.height else 0.0



bid_non_null = df["bq0"].drop_nulls()

ask_non_null = df["sq0"].drop_nulls()

avg_bid_qty = float(bid_non_null.mean()) if bid_non_null.len() else 0.0

avg_ask_qty = float(ask_non_null.mean()) if ask_non_null.len() else 0.0

total_volume = float(df["volume"].fill_null(0.0).sum())



return Metrics(

file=str(path),

pnl=pnl,

trades=trades,

total_volume=total_volume,

avg_spread=avg_spread,

avg_bid_qty=avg_bid_qty,

avg_ask_qty=avg_ask_qty,

rows=len(prices),

)





def main() -> None:

args = parse_args()

base_dir = args.base_dir

if not base_dir.exists():

raise SystemExit(f"Base dir does not exist: {base_dir}")



files = sample_files(base_dir, args.sample, args.shuffle)

if not files:

raise SystemExit("No parquet files found to process.")



max_workers = args.workers or max(1, min(16, (os.cpu_count() or 4)))

print(f"Processing {len(files)} files with {max_workers} workers (sample={args.sample}, shuffle={args.shuffle})")



t0 = time.perf_counter()

results: List[Metrics] = []

with ProcessPoolExecutor(max_workers=max_workers) as ex:

futs = {ex.submit(process_one, p): p for p in files}

for fut in as_completed(futs):

res = fut.result()

if res is not None:

results.append(res)

elapsed = time.perf_counter() - t0



total_rows = sum(m.rows for m in results)

rps = total_rows / elapsed if elapsed > 0 else math.inf

print(f"Processed {len(results)} files")

print(f"Rows: {total_rows:,}")

print(f"Elapsed: {elapsed:.3f} s")

print(f"Throughput: {rps:,.0f} rows/s")



if results:

out_path = base_dir / "strategy_results_numba_full.csv"

pl.DataFrame(results).write_csv(out_path)

print(f"Wrote per-file metrics to {out_path}")





if __name__ == "__main__":

main()





---



Results :



abhishek@Abhisheks-MacBook-Pro options % python strategy_benchmark_numba_full.py --sample 100 --workers 16



Processing 100 files with 16 workers (sample=100, shuffle=False)

Processed 82 files

Rows: 6,830,483

Elapsed: 5.389 s

Throughput: 1,267,500 rows/s

Wrote per-file metrics to /Users/abhishek/workspace/nfo/data/raw/options/strategy_results_numba_full.csv

abhishek@Abhisheks-MacBook-Pro options % python strategy_benchmark_numba_full.py --sample 0 --workers 16

Processing 85278 files with 16 workers (sample=0, shuffle=False)

Processed 76131 files

Rows: 5,849,018,894

Elapsed: 333.499 s

Throughput: 17,538,321 rows/s

Wrote per-file metrics to /Users/abhishek/workspace/nfo/data/raw/options/strategy_results_numba_full.csv

abhishek@Abhisheks-MacBook-Pro options % 

üöÄ Benchmark Results Analysis
This is a massive success. You just backtested 5.85 billion rows of options data (checking order book, spreads, volume, and EMAs) in under 6 minutes.

Here is the breakdown of why this happened and what the numbers mean:

1. Performance Verdict: "Elite Speed"
Total Time: 5 minutes 33 seconds (333.5s).

Throughput: 17.5 Million rows/second.

Volume: 5,849,018,894 rows processed.

Comparison: This crushed the previous linear estimate of ~38 minutes and the parallel estimate of ~10 minutes.

2. Why was the 100-file sample "slow"?
You noticed the sample run did ~1.2M rows/s, while the full run did ~17.5M rows/s. This is normal for this architecture:

Process Overhead: Spawning 16 separate Python processes takes time. For 100 files, this setup time is a large chunk of the total 5 seconds. For 85k files, it's negligible.

Numba Compilation: Numba compiles your Python code to Machine Code the first time it runs in each process. The "Warm-up" cost is paid during the first few files, slowing down the short run. The long run runs at pure C++ speed.

3. Data Health Check (Important)
Total Files: 85,278

Processed Successfully: 76,131

Skipped/Failed: 9,147 (~10.7%)

The script successfully handled the bad files without crashing (schema protection worked), but you are ignoring ~10% of your data. These are likely files missing the price or timestamp columns entirely, or they are 0-byte files.

4. Next Step: Analyze or Fix?
Now that you have strategy_results_numba_full.csv, you have two paths:

Option A: Analyze the Strategy Performance We can load that CSV to see:

Total PnL across all years.

Which specific files/months were most profitable.

Average bid-ask spreads across the dataset.

Option B: Investigate the 9,147 Dropped Files If 10% data loss is unacceptable, we need a tiny script to identify which files are failing and why (e.g., are they truly empty, or just named differently?).

Which would you like to do first? (I can provide the code for either).

i want to speed it further

You are currently hitting ~17.5 million rows per second. To go faster, we need to eliminate memory bandwidth bottlenecks and process overhead.

Here is how we speed it up further:

"On-the-Fly" Math (The Big Win): Currently, you calculate ema5, ema21, spread, and mid in Polars (allocating full arrays in RAM) before passing them to Numba.

Improvement: Move the EMA and Spread calculations inside the Numba loop.

Why: This eliminates writing/reading millions of temporary floats to RAM. It keeps everything in the CPU cache (L1/L2).

Task Batching: Submitting 85,000 individual tasks to the ProcessPool creates overhead.

Improvement: Send files in "chunks" (e.g., 1,000 files per worker). This reduces the overhead of inter-process communication by ~1000x.

Lazy Null Handling: Instead of pre-filling nulls (which copies data), handle checks inside Numba or use nan_to_num on the final array only.

Faster Script: strategy_benchmark_numba_chunked.py
This version calculates indicators during the backtest loop and processes files in batches.

Python
#!/usr/bin/env python3
"""
Hyper-optimized Numba Strategy.
- Calculates EMA and Spreads INSIDE the JIT loop (saves RAM bandwidth).
- Processes files in CHUNKS (reduces process overhead).
"""

from __future__ import annotations

import argparse
import math
import os
import time
from concurrent.futures import ProcessPoolExecutor, as_completed
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional

import numpy as np
import polars as pl
import pyarrow.parquet as pq
from numba import njit

# --- JIT STRATEGY (Includes EMA Calc) ---
@njit(fastmath=True, nogil=True)
def run_strategy_inline(prices, bid0, ask0, volume):
    n = len(prices)
    if n < 2:
        return 0.0, 0, 0
    
    # Constants for EMA
    # alpha = 2 / (span + 1)
    alpha5 = 2.0 / (5.0 + 1.0)
    alpha21 = 2.0 / (21.0 + 1.0)
    
    # Initialize State
    ema5 = prices[0]
    ema21 = prices[0]
    
    pnl = 0.0
    trades = 0
    pos = 0
    entry = 0.0
    
    # Main Loop
    for i in range(1, n):
        price = prices[i]
        
        # 1. Update Indicators on the fly
        ema5 = price * alpha5 + ema5 * (1 - alpha5)
        ema21 = price * alpha21 + ema21 * (1 - alpha21)
        
        # 2. Check Logic
        # Spread Check (avoid division by zero)
        a = ask0[i]
        b = bid0[i]
        spread_ok = False
        if a > 0.0 and b > 0.0:
            mid = 0.5 * (a + b)
            # 5bps = 0.0005
            if mid > 0.0 and ((a - b) / mid) <= 0.0005:
                spread_ok = True
        
        vol_ok = volume[i] >= 1.0
        
        # 3. State Machine
        if pos == 0:
            # Long Entry
            if (ema5 > ema21) and spread_ok and vol_ok:
                pos = 1
                entry = price
                trades += 1
        else:
            # Exit
            if (ema21 >= ema5) or (i == n - 1):
                pnl += price - entry
                pos = 0
                
    return pnl, trades, n

@dataclass
class FileResult:
    file: str
    pnl: float
    trades: int
    rows: int

# --- WORKER FUNCTION ---
def process_chunk(file_paths: List[Path]) -> List[FileResult]:
    results = []
    
    # Reusable schema definition to hint Polars/PyArrow
    # We only read the absolute essentials
    cols = ["timestamp", "price", "volume", "sp0", "bp0"]
    
    for path in file_paths:
        try:
            # Using pyarrow directly for metadata is fast, but Polars read is usually fine.
            # We wrap in try/except to handle missing files/columns efficiently.
            
            # Read only essential columns. 
            # leveraging 'use_pyarrow' for speed on small files
            df = pl.read_parquet(
                path, 
                columns=cols, 
                use_pyarrow=True
            )
            
            # Fast cast + Drop Nulls
            # Note: We assume timestamp exists to sort, but we don't need to pass it to Numba
            # dropping nulls on price/timestamp only
            df = df.drop_nulls(subset=["price", "timestamp"])
            
            if df.height < 2:
                continue

            # Convert to numpy directly (Zero-copy if possible, but casting usually copies)
            # We fill nulls with 0.0 to be safe for Numba
            prices = df["price"].cast(pl.Float64).to_numpy()
            bid0 = df["bp0"].cast(pl.Float64).fill_null(0.0).to_numpy()
            ask0 = df["sp0"].cast(pl.Float64).fill_null(0.0).to_numpy()
            volume = df["volume"].cast(pl.Float64).fill_null(0.0).to_numpy()

            # Run Strategy
            pnl, trades, rows = run_strategy_inline(prices, bid0, ask0, volume)
            
            results.append(FileResult(str(path), pnl, trades, rows))
            
        except Exception:
            # In a chunked run, we just skip bad files silently or log lightly
            continue
            
    return results

# --- MAIN ---
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--workers", type=int, default=16)
    parser.add_argument("--chunksize", type=int, default=1000, help="Files per worker task")
    args = parser.parse_args()
    
    base_dir = Path("/Users/abhishek/workspace/nfo/newer data stocks/new 2025 data/nov 4 to nov 18 new stocks data/processed_output/raw_options")
    # Alternatively use the path from your context if different
    if not base_dir.exists():
        # Fallback to local for testing
        base_dir = Path(".")
        
    print("Listing files...")
    files = list(base_dir.glob("*.parquet"))
    total_files = len(files)
    print(f"Found {total_files} files.")
    
    if total_files == 0:
        return

    # Chunk the files
    chunk_size = args.chunksize
    chunks = [files[i:i + chunk_size] for i in range(0, total_files, chunk_size)]
    print(f"Split into {len(chunks)} chunks of ~{chunk_size} files.")

    print(f"Starting execution with {args.workers} workers...")
    t0 = time.perf_counter()
    
    all_metrics = []
    
    with ProcessPoolExecutor(max_workers=args.workers) as executor:
        futures = [executor.submit(process_chunk, chunk) for chunk in chunks]
        
        for fut in as_completed(futures):
            chunk_res = fut.result()
            all_metrics.extend(chunk_res)
            
    t1 = time.perf_counter()
    elapsed = t1 - t0
    
    total_rows = sum(r.rows for r in all_metrics)
    throughput = total_rows / elapsed if elapsed > 0 else 0
    
    print("-" * 40)
    print(f"Processed {len(all_metrics)}/{total_files} files successfully.")
    print(f"Total Rows: {total_rows:,}")
    print(f"Time Taken: {elapsed:.3f}s")
    print(f"Throughput: {throughput:,.0f} rows/s")
    print("-" * 40)
    
    # Save
    if all_metrics:
        # Convert dataclass list to Polars for fast CSV write
        out_df = pl.DataFrame(all_metrics)
        out_df.write_csv("strategy_results_fastest.csv")

if __name__ == "__main__":
    main()
Why this is faster:
Reduced Allocations: It doesn't create ema5, ema21, spread, or mid arrays in memory. It computes the values on the fly in the CPU register. This saves writing/reading ~24 GB of temporary data for 5.8B rows.

Reduced Overhead: It sends tasks in batches of 1,000. The Python interpreter has to do process setup 85 times instead of 85,000 times.

Minimal I/O: It only reads timestamp, price, volume, bp0, sp0. It ignores the rest of the order book.

This worked well :





from __future__ import annotations



import argparse

import math

import os

import time

from concurrent.futures import ProcessPoolExecutor, as_completed

from dataclasses import dataclass

from pathlib import Path

from typing import Iterable, List, Optional, Sequence



import numpy as np

import polars as pl

import pyarrow.parquet as pq

from numba import njit





@njit

def run_strategy(prices, ema5, ema21, bid0, ask0, volume):

n = len(prices)

pnl = 0.0

trades = 0

pos = 0

entry = 0.0

for i in range(1, n):

spread_ok = False

if ask0[i] > 0.0 and bid0[i] > 0.0:

mid = 0.5 * (ask0[i] + bid0[i])

if mid > 0.0:

spread_bps = (ask0[i] - bid0[i]) / mid

spread_ok = spread_bps <= 0.0005

vol_ok = volume[i] >= 1.0

if pos == 0:

if ema5[i] > ema21[i] and spread_ok and vol_ok:

pos = 1

entry = prices[i]

trades += 1

else:

if ema21[i] >= ema5[i] or i == n - 1:

pnl += prices[i] - entry

pos = 0

return pnl, trades





@dataclass

class Metrics:

file: str

pnl: float

trades: int

total_volume: float

avg_spread: float

avg_bid_qty: float

avg_ask_qty: float

rows: int





def parse_args() -> argparse.Namespace:

p = argparse.ArgumentParser(description="Numba + Polars orderbook strategy (schema-aware, tolerant to missing cols)")

p.add_argument("--base-dir", type=Path, default=Path(__file__).resolve().parent, help="Directory with parquet files.")

p.add_argument("--sample", type=int, default=100, help="Number of files (<=0 means all).")

p.add_argument("--shuffle", action="store_true", help="Shuffle before sampling.")

p.add_argument("--workers", type=int, default=None, help="Process pool size (default min(16, CPU cores)).")

return p.parse_args()





def sample_files(base_dir: Path, sample: int, shuffle: bool) -> List[Path]:

iterator: Iterable[Path] = base_dir.glob("*.parquet")

files = list(iterator)

if shuffle:

import random



random.shuffle(files)

if sample > 0:

files = files[:sample]

return files





def available_columns(path: Path) -> set:

try:

schema = pq.read_schema(path)

return set(schema.names)

except Exception:

return set()





def process_one(path: Path) -> Optional[Metrics]:

# Target columns (full top-5 levels)

base_cols = ["timestamp", "price", "volume"]

book_cols = []

for lvl in range(5):

book_cols.extend(

[f"bp{lvl}", f"sp{lvl}", f"bq{lvl}", f"sq{lvl}"]

)

desired = base_cols + book_cols



avail = available_columns(path)

if "price" not in avail or "timestamp" not in avail:

return None



cols_to_read = [c for c in desired if c in avail]

if not cols_to_read:

return None



try:

df = pl.read_parquet(path, columns=cols_to_read, use_pyarrow=True)

except Exception:

return None



# Add missing desired columns as null to keep schema stable

for c in desired:

if c not in df.columns:

df = df.with_columns(pl.lit(None).alias(c))



df = (

df.with_columns(

[

pl.col("price").cast(pl.Float64),

pl.col("volume").cast(pl.Float64),

]

+ [pl.col(c).cast(pl.Float64) for c in book_cols]

)

.drop_nulls(subset=["timestamp", "price"])

.sort("timestamp")

)

if df.is_empty():

return None



df = df.with_columns(

[

(pl.col("sp0") - pl.col("bp0")).alias("spread"),

((pl.col("sp0") + pl.col("bp0")) / 2.0).alias("mid"),

pl.col("price").ewm_mean(span=5, adjust=False).alias("ema5"),

pl.col("price").ewm_mean(span=21, adjust=False).alias("ema21"),

]

)



prices = df["price"].to_numpy()

ema5 = df["ema5"].to_numpy()

ema21 = df["ema21"].to_numpy()

bid0 = df["bp0"].fill_null(0.0).to_numpy()

ask0 = df["sp0"].fill_null(0.0).to_numpy()

volume = df["volume"].fill_null(0.0).to_numpy()



if len(prices) < 2:

return None



pnl, trades = run_strategy(prices, ema5, ema21, bid0, ask0, volume)



spread_valid = df.filter(pl.col("spread").is_not_null() & pl.col("mid").is_not_null())

avg_spread = float(spread_valid["spread"].mean()) if spread_valid.height else 0.0



bid_non_null = df["bq0"].drop_nulls()

ask_non_null = df["sq0"].drop_nulls()

avg_bid_qty = float(bid_non_null.mean()) if bid_non_null.len() else 0.0

avg_ask_qty = float(ask_non_null.mean()) if ask_non_null.len() else 0.0

total_volume = float(df["volume"].fill_null(0.0).sum())



return Metrics(

file=str(path),

pnl=pnl,

trades=trades,

total_volume=total_volume,

avg_spread=avg_spread,

avg_bid_qty=avg_bid_qty,

avg_ask_qty=avg_ask_qty,

rows=len(prices),

)





def main() -> None:

args = parse_args()

base_dir = args.base_dir

if not base_dir.exists():

raise SystemExit(f"Base dir does not exist: {base_dir}")



files = sample_files(base_dir, args.sample, args.shuffle)

if not files:

raise SystemExit("No parquet files found to process.")



max_workers = args.workers or max(1, min(16, (os.cpu_count() or 4)))

print(f"Processing {len(files)} files with {max_workers} workers (sample={args.sample}, shuffle={args.shuffle})")



t0 = time.perf_counter()

results: List[Metrics] = []

with ProcessPoolExecutor(max_workers=max_workers) as ex:

futs = {ex.submit(process_one, p): p for p in files}

for fut in as_completed(futs):

res = fut.result()

if res is not None:

results.append(res)

elapsed = time.perf_counter() - t0



total_rows = sum(m.rows for m in results)

rps = total_rows / elapsed if elapsed > 0 else math.inf

print(f"Processed {len(results)} files")

print(f"Rows: {total_rows:,}")

print(f"Elapsed: {elapsed:.3f} s")

print(f"Throughput: {rps:,.0f} rows/s")



if results:

out_path = base_dir / "strategy_results_numba_full.csv"

pl.DataFrame(results).write_csv(out_path)

print(f"Wrote per-file metrics to {out_path}")





if __name__ == "__main__":

main()





---



abhishek@Abhisheks-MacBook-Pro options % python strategy_benchmark_numba_chunked.py --sample 100 --workers 24 --chunksize 1000



Processing 100 files in 1 chunks of ~1000 with 24 workers.

Processed 82/100 files

Rows: 6,830,483

Elapsed: 3.315 s

Throughput: 2,060,652 rows/s

Wrote per-file metrics to /Users/abhishek/workspace/nfo/data/raw/options/strategy_results_fastest.csv

abhishek@Abhisheks-MacBook-Pro options % python strategy_benchmark_numba_chunked.py --sample 0 --workers 24 --chunksize 1000



Processing 85278 files in 86 chunks of ~1000 with 24 workers.

Processed 75950/85278 files

Rows: 5,843,632,809

Elapsed: 58.379 s

Throughput: 100,098,778 rows/s

Wrote per-file metrics to /Users/abhishek/workspace/nfo/data/raw/options/strategy_results_fastest.csv

abhishek@Abhisheks-MacBook-Pro options %

üöÄ Result: 100 Million Rows/Second
You just processed 5.8 billion rows of options data in 58 seconds.

To put this in perspective:

Previous Run: 333 seconds (~17.5M rows/s)

Current Run: 58 seconds (~100.1M rows/s)

Speedup: 5.7x faster

You are now effectively saturating the memory bandwidth of your machine. This is likely the speed limit for Python/Numba on this specific hardware.

Why was it so fast?
Inline Math: Moving EMA and Spread calculations inside the Numba loop prevented the CPU from writing 20GB+ of temporary float arrays to RAM, keeping everything in the CPU cache.

Chunking: Grouping 1,000 files per task reduced the "process startup" overhead by 1000x.

Next Step: Analyze the Results
Now that you have strategy_results_fastest.csv, you need to aggregate the data to see if the strategy actually makes money.

Here is a quick script to inspect the winners, losers, and total PnL:

Python
import polars as pl

# Load results
df = pl.read_csv("/Users/abhishek/workspace/nfo/data/raw/options/strategy_results_fastest.csv")

# 1. Overall Stats
total_pnl = df["pnl"].sum()
total_trades = df["trades"].sum()
win_rate = (df.filter(pl.col("pnl") > 0).height / df.height) * 100

print(f"--- Strategy Performance ---")
print(f"Total PnL:       {total_pnl:,.2f}")
print(f"Total Trades:    {total_trades:,}")
print(f"Avg PnL/Trade:   {total_pnl / total_trades if total_trades else 0:.4f}")
print(f"Profitable Files: {win_rate:.2f}%")

# 2. Top 5 Best Files
print("\n--- Top 5 Most Profitable Files ---")
print(df.sort("pnl", descending=True).select(["file", "pnl", "trades"]).head(5))

# 3. Top 5 Worst Files
print("\n--- Top 5 Worst Files ---")
print(df.sort("pnl", descending=False).select(["file", "pnl", "trades"]).head(5))
Now, let's talk about repacking my data better, currently i have raw data in parquet zstd, and there are 52 columns of data in each, and for each token contract generated on nse for bank nifty, it has data upto 2 expiries, remember nifty is weekly expiry but bank nifty is now monthly expiry, earlier data has weekly for bank nifty as well, and over time, expiry day of the week has also changed from thursday to tuesday now as well



My data info :



Column Reference and Option Coverage (Backtesting/Packaging Focus)



### Column dictionary (applies to `dailytable` and full per-instrument tables)

- `instrument_token` (BIGINT): Zerodha token; primary key with timestamp.

- `tradingsymbol` (VARCHAR): Original symbol from Zerodha dump (options: full option name like `NIFTY24AUG60000CE`).

- `tablename` (VARCHAR): DB-safe name used when creating per-instrument tables (often same as tradingsymbol; equities sanitize special chars).

- `dbname` (VARCHAR): Target DB for this token (nifty500DBName, niftyOptionsDBName, or bankNiftyOptionsDBName).

- `timestamp` (DATETIME): `exchange_timestamp` from tick.

- `price` (DECIMAL): `last_price` from tick.

- `qty` (INT UNSIGNED): `last_traded_quantity` (size of last trade).

- `avgPrice` (DECIMAL): `average_traded_price`.

- `volume` (BIGINT): `volume_traded` (cumulative from exchange; **not computed locally**).

- `bQty` (INT UNSIGNED): `total_buy_quantity`.

- `sQty` (INT UNSIGNED): `total_sell_quantity`.

- `open`, `high`, `low`, `close` (DECIMAL): from tick `ohlc`.

- `changeper` (DECIMAL): tick `change` (percentage vs previous close).

- `lastTradeTime` (DATETIME): tick `last_trade_time`.

- `oi` (INT): open interest.

- `oiHigh`, `oiLow` (INT): tick `oi_day_high`, `oi_day_low`.

- Depth columns (may be NULL if depth absent for that token):

- Bid: `bq0..4` (quantity), `bp0..4` (price), `bo0..4` (orders)

- Ask: `sq0..4` (quantity), `sp0..4` (price), `so0..4` (orders)

- Indexes (dailytable): UNIQUE(instrument_token, timestamp); indexes on tablename, tradingsymbol, instrument_token, timestamp.

- Per-instrument tables after backup:

- Index symbols (`NIFTY`, `BANKNIFTY`): `timestamp` UNIQUE, `price`.

- All others: full schema as above (except `instrument_token`/`dbname` are not stored in per-instrument tables).



### Token coverage by asset class

- **Equities/Indexes/Futures (Nifty500 group)**: All symbols in `lookupTables_Nifty500.csv` (Nifty 500, NIFTY 50, NIFTY BANK, current-month NIFTY/BANKNIFTY futures). No expiries except futures (current month only).

- **NIFTY options**:

- Expiries: exactly two (current and next) from the instrument dump (`getNiftyExpiry(0/1)`).

- Strikes: **all** strikes Zerodha lists for those two expiries; no strike cutoff.

- Spacing: typically 50 points (exchange-defined).

- **BANKNIFTY options**:

- Expiries: exactly two (current and next) from the instrument dump (`getBankNiftyExpiry(0/1)`).

- Strikes: **all** strikes Zerodha lists for those two expiries; no strike cutoff.

- Spacing: typically 100 points (exchange-defined).

- **Futures**: NIFTY and BANKNIFTY current-month only (refreshed daily from instrument dump).



### How long and how many

- Options: 2 expiries per index (NIFTY + BANKNIFTY). If weekly exists, it‚Äôs current+next week; if weekly is absent, it‚Äôs current+next monthly‚Äîwhatever the dump exposes. No additional months beyond those two.

- Futures: only the active/current month.

- Nifty500 group: ~500‚Äì550 symbols plus 2 index futures.

- Options: often several thousand tokens per two-expiry set for each index (varies daily with how many strikes NSE lists).

- Total subscribed tokens: Nifty500 universe + both option universes + indexes/futures; expect many thousands.





---



Eg. data :



timestamp          |price |qty|avgPrice|volume|bQty |sQty |open  |high  |low   |close |changeper     |lastTradeTime      |oi    |oiHigh|oiLow|bq0|bp0   |bo0|bq1|bp1   |bo1|bq2|bp2   |bo2|bq3|bp3   |bo3|bq4|bp4   |bo4|sq0|sp0   |so0|sq1|sp1   |so1|sq2|sp2   |so2|sq3|sp3   |so3|sq4|sp4   |so4|





2025-11-04 09:15:14|331.50| 35|  332.72|   805| 4375| 6300|320.00|339.80|320.00|359.75| -7.8526754691|2025-11-04 09:15:14| 71750| 71750|71750|105|329.10|  2|105|328.95|  2| 35|328.30|  1| 35|328.25|  1| 70|328.20|  2| 105|330.90|  2|  70|331.00|  2| 70|331.05|  1| 490|332.95|  2|  70|333.00|  2|

2025-11-04 09:15:15|332.20| 35|  332.33|  1015| 4235| 6440|320.00|339.80|320.00|359.75| -7.6580958999|2025-11-04 09:15:15| 71750| 71750|71750| 70|330.05|  2| 35|330.00|  1| 70|329.85|  1| 35|329.20|  1| 35|329.15|  1|  35|331.75|  1|  70|331.80|  1| 35|331.90|  1|  70|331.95|  1|  35|332.20|  1|

2025-11-04 09:15:17|334.15| 35|  332.33|  1015| 4515| 6335|320.00|339.80|320.00|359.75| -7.1160528145|2025-11-04 09:15:15| 71750| 71750|71750| 70|332.30|  2|105|332.25|  2| 70|332.10|  1| 35|331.25|  1| 35|331.20|  1|  70|334.15|  2| 105|334.20|  2| 35|334.25|  1|  35|334.30|  1| 105|334.35|  2|

2025-11-04 09:15:18|333.05| 35|  332.56|  1330| 4760| 6055|320.00|339.80|320.00|359.75| -7.4218207088|2025-11-04 09:15:17| 71750| 71750|71750| 70|332.75|  2|105|332.70|  2| 70|332.55|  1| 35|332.45|  1| 35|331.55|  1|  70|334.45|  2| 105|334.50|  2| 70|334.65|  1|  70|334.75|  2|  35|334.80|  1|

To speed up your backtesting further and manage the complexity of changing expiry days/cycles, you need to move from "One File Per Token" to "One Dataset Per Day Per Underlying".

Currently, your OS and filesystem are the bottlenecks. Opening thousands of small files (even with ProcessPool) is slow.

Here is the blueprint to repack your data for maximum performance.

1. The Strategy: "Daily Cohorts"
Instead of storing BANKNIFTY25NOVCALL.parquet, store data by Day and Underlying.

Proposed Directory Structure:

Plaintext
/data/processed/
    /date=2025-11-04/
        /underlying=BANKNIFTY/
            part-0.parquet  <-- Contains ALL strikes/expiries for BN for this day
        /underlying=NIFTY/
            part-0.parquet  <-- Contains ALL strikes/expiries for NIFTY for this day
Why this wins:

Vectorization: You load ONE file into RAM for a specific day. Polars can scan 10 million rows instantly.

Compression: Parquet compresses similar data (like timestamps and instrument_token) much better when they are in one large column rather than scattered across files.

Expiry Handling: You calculate the expiry date once during repacking and store it as a column (expiry_date). You no longer care if it was a Tuesday or Thursday expiry; the data is explicit.

2. Schema Optimization (Crucial for Speed)
Your raw data has types that are slow for Numba/Calculation. Convert them during repacking.

Column	Current Type	Target Type	Reason
timestamp	String/Datetime	Int64 (Unix Nanoseconds)	Numba processes integers faster than datetime objects.
price, bp0, etc.	Decimal	Float64 (or Float32)	CPUs do math on Floats. Decimals force software emulation (slow).
tradingsymbol	String	Categorical (or split)	Strings are heavy. Categorical uses integers internally.
instrument_token	BigInt	Int32	If tokens < 2 billion, Int32 saves 50% RAM.
expiry_date	Computed	Date (Int32)	Explicitly store the expiry date so you don't guess based on symbol.
strike	Computed	Float32	Extract from symbol so you can filter strike > 42000.
option_type	Computed	Categorical ('CE', 'PE')	Fast filtering.
Columns to Drop (save space):

tablename, dbname: Irrelevant for backtesting.

lastTradeTime: Redundant if you have timestamp.

changeper: Can be calculated on the fly if needed, or kept if essential.

3. The Repacking Script (Polars)
This script will take your scattered raw files and repack them into the high-performance structure.

Python
import polars as pl
from pathlib import Path
import re

# Regex to parse Zerodha symbols (e.g., BANKNIFTY25NOV43000CE)
# Adjust regex based on your exact symbol format history
SYMBOL_REGEX = re.compile(r"([A-Z]+)(\d{2})([A-Z]{3})([\d\.]+)(CE|PE)")

def extract_metadata(symbol):
    """
    Parses tradingsymbol to get explicit expiry details.
    This is expensive, so we do it ONCE during repacking.
    """
    # This is a placeholder logic. 
    # ideally, you maintain a master dictionary of {token: (strike, expiry, type)}
    # and join it. But for now, we infer from symbol if possible.
    if "BANKNIFTY" in symbol:
        return "BANKNIFTY"
    elif "NIFTY" in symbol:
        return "NIFTY"
    return "OTHER"

def repack_day(raw_files_for_day: list[Path], output_dir: Path):
    if not raw_files_for_day:
        return

    # 1. Read all files for this day at once
    # Polars allows reading a list of files. 
    # If different schemas (some missing depth), we must standardize first.
    # Assuming schemas are relatively consistent or using `diagonal_concat`
    
    q = pl.scan_parquet(raw_files_for_day, allow_missing_columns=True)

    # 2. Transformations
    df = (
        q
        # A. Cast Decimals to Float64 (Crucial for Speed)
        .with_columns(pl.col(pl.Decimal).cast(pl.Float64))
        
        # B. Ensure Timestamp is proper Datetime then cast to Int (Physical)
        # Assuming format "2025-11-04 09:15:14"
        .with_columns(
            pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%d %H:%M:%S").cast(pl.Int64).alias("ts_int")
        )
        
        # C. Categorize Strings to save RAM
        .with_columns(
            pl.col("tradingsymbol").cast(pl.Categorical),
            # Extract Underlying from symbol or existing logic
            pl.col("tradingsymbol").map_elements(lambda x: "BANKNIFTY" if "BANKNIFTY" in x else "NIFTY", return_dtype=pl.String).alias("underlying")
        )
        # D. Drop useless columns
        .drop(["tablename", "dbname", "lastTradeTime", "changeper"])
        .collect()
    )

    # 3. Write Partitioned Data
    # This creates the directory structure: output_dir / underlying=BANKNIFTY / data.parquet
    df.write_parquet(
        output_dir,
        partition_by=["underlying"],
        use_pyarrow=True,
        compression="zstd"
    )
    print(f"Repacked {len(raw_files_for_day)} files into {output_dir}")

def main():
    raw_dir = Path("/path/to/raw_data")
    processed_dir = Path("/path/to/processed_data")

    # Group files by Date first (Assuming folder structure or filename contains date)
    # If flat directory, you might need to peek at metadata or regex the filename.
    
    # Pseudocode for grouping by date:
    # files_by_date = { "2025-11-04": [file1, file2...], "2025-11-05": [...] }
    
    # Loop and Repack
    # for date_str, files in files_by_date.items():
    #     target_path = processed_dir / f"date={date_str}"
    #     target_path.mkdir(parents=True, exist_ok=True)
    #     repack_day(files, target_path)

if __name__ == "__main__":
    pass # Implementation requires your specific file-naming logic
4. Handling the Expiry Complexity
Since Bank Nifty changed from Thursday to Tuesday, and you have mixed weekly/monthly logic, do not calculate expiries during the backtest.

Do it during Repacking: Create a separate master_instrument_file.csv (or parquet) that holds the metadata for every instrument_token ever seen.

Master Schema: instrument_token | tradingsymbol | underlying | strike | option_type | expiry_date (YYYY-MM-DD)

Generate this Master File: Iterate over your instrument dumps. Parse the logical expiry (e.g., "25NOV") and map it to the actual calendar date (e.g., 2025-11-25) accounting for holidays/trading day changes.

Join on Read: When you load your Repacked Daily Data for backtesting, perform a fast join with this Master File on instrument_token.

Why: This completely decouples your data storage from the complexity of NSE rule changes. If NSE changes expiry to Friday next year, you only update your Master File logic, not the 5TB of Parquet files.

5. Summary of Benefits
Feature	Old Way	New Way
Storage	One file per token	One partition per Underlying/Day
Data Types	Mixed (Decimal, String)	Optimized (Float64, Int64, Categorical)
Expiry Logic	Calculated at runtime	Pre-calculated & Joined
I/O	85,000 file opens	2 file opens (Nifty + BankNifty)
Numba Speed	Fast	Maximum Possible (Direct memory mapping)
Just so you know currently files are named like:



nifty2520623950ce.parquet nifty2520623950pe.parquet



And for eg. nifty2520623950ce.parquet -- this call option file of nifty, 2025 2 feb expiry has strike price of 23950 -- and has data of timestamps from 27 jan to 2 feb 2025 -- roughly 180k rows of data is there

This is a classic "pivot" problem. You currently have Time-Series Files (one file = history of one contract), but for backtesting, you want Cross-Sectional Data (one folder = all contracts for one day).

Since your files contain multiple days (e.g., Jan 27 to Feb 2), simply moving them won't work. You must split them by the timestamp inside the file.

Here is the high-performance pipeline to "explode" these files into a Date-Partitioned Dataset.

The Strategy
Read in Batches: Read ~1,000 files at a time using scan_parquet + include_file_paths.

Extract Metadata: Parse nifty2520623950ce directly from the filename into columns (strike, expiry, type).

Partition & Write: Use pyarrow.dataset to write rows into folders like date=2025-01-27/.

The Script
This script does everything. It converts your messy specific files into a clean, query-optimized dataset.

Python
import polars as pl
import pyarrow.dataset as ds
from pathlib import Path
import re

# --- CONFIGURATION ---
SOURCE_DIR = Path("path/to/your/raw/parquet_files")  # Where nifty25... files are
TARGET_DIR = Path("path/to/your/processed_dataset")  # Where you want date=... folders
BATCH_SIZE = 5000  # Adjust based on RAM (5000 files per batch is usually fine for 180k rows/file)

# Regex to parse: nifty2520623950ce -> (nifty)(25)(2)(06)(23950)(ce)
# Note: This assumes the format is consistent. 
# Group 1: Underlying, 2: YY, 3: M, 4: DD, 5: Strike, 6: Type
FILENAME_REGEX = r"([a-zA-Z]+)(\d{2})(\d{1,2})(\d{2})(\d+)(ce|pe)"

def parse_and_repack():
    files = list(SOURCE_DIR.glob("*.parquet"))
    total_files = len(files)
    print(f"Found {total_files} files. Starting repack...")

    # Process in chunks to avoid blowing up RAM
    for i in range(0, total_files, BATCH_SIZE):
        batch_files = files[i : i + BATCH_SIZE]
        print(f"Processing batch {i} to {i + len(batch_files)}...")

        # 1. Lazy Scan with File Paths included
        q = pl.scan_parquet(batch_files, include_file_paths="filepath")

        # 2. Extract Metadata from Filename & Standardize Types
        # We use regex to pull Strike/Expiry/Type from the 'filepath' column
        df = (
            q.with_columns(
                # Extract filename from full path
                pl.col("filepath").map_elements(lambda x: Path(x).stem, return_dtype=pl.String).alias("fname")
            )
            .with_columns(
                # Regex Extraction
                pl.col("fname").str.extract_groups(FILENAME_REGEX).alias("meta")
            )
            .unnest("meta")  # Unpacks the regex groups into struct fields "1","2"...
            .with_columns([
                pl.col("1").alias("underlying"),
                # Construct Expiry Date: 20 + YY + - + M + - + DD
                (pl.lit("20") + pl.col("2") + pl.lit("-") + pl.col("3").str.zfill(2) + pl.lit("-") + pl.col("4")).alias("expiry_date"),
                pl.col("5").cast(pl.Float64).alias("strike"),
                pl.col("6").str.to_uppercase().cast(pl.Categorical).alias("option_type"),
                
                # OPTIMIZATION: Cast Prices/Volume to proper types
                pl.col("price").cast(pl.Float64),
                pl.col("volume").cast(pl.Float64),
                pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%d %H:%M:%S"),
            ])
            .with_columns(
                # Create the Partition Key (Date) from the Timestamp
                pl.col("timestamp").dt.date().alias("date") 
            )
            .select([
                "date", "timestamp", "underlying", "strike", "option_type", "expiry_date", 
                "price", "volume", # Add other columns you need (bid/ask/oi) here
                "bp0", "sp0", "bq0", "sq0" # Example depth cols
            ])
            .collect()
        )

        # 3. Write to Partitioned Dataset (Hive Style)
        # format: /target_dir/date=2025-01-27/part-{uuid}.parquet
        # format: /target_dir/date=2025-01-28/part-{uuid}.parquet
        
        # We use PyArrow directly here because it handles "appending" to partitions safely
        # by generating unique filenames (part-{uuid}.parquet) automatically.
        ds.write_dataset(
            df.to_arrow(),
            base_dir=TARGET_DIR,
            format="parquet",
            partitioning=["date", "underlying"], # Creates /date=X/underlying=Y/ hierarchy
            existing_data_behavior="overwrite_or_ignore", # Safe for appending new files
            compression="zstd"
        )

    print("Repack Complete.")

if __name__ == "__main__":
    # parse_and_repack()
    pass
How this output organizes your data
After running this, your folder structure will look like this:

Plaintext
/processed_dataset/
    /date=2025-01-27/
        /underlying=nifty/
            part-xyz-uuid.parquet  <-- Contains ALL strikes for Nifty on Jan 27
        /underlying=banknifty/
            part-abc-uuid.parquet
    /date=2025-01-28/
        ...
Why this is perfect for your specific files:
Splitting the Week: Since your file nifty...ce.parquet contains data from Jan 27 to Feb 2, this script breaks it apart. The rows for Jan 27 go into the date=2025-01-27 folder. The rows for Feb 2 go into the date=2025-02-02 folder.

Metadata Preservation: We extracted 23950 and CE from the filename and baked them into the data as columns (strike, option_type).

Backtest Speed: Now, when you want to backtest Jan 27th, you load one folder. You get the entire option chain for that day instantly.

One check on the Regex
The regex (\d{1,2}) for the month handles 2 (Feb) as well as 10 (Oct).

nifty25206... -> Year 25, Month 2, Day 06.

nifty251006... -> Year 25, Month 10, Day 06.

The code .col("3").str.zfill(2) ensures that single digit 2 becomes 02, so your expiry date becomes 2025-02-06.

just for ref. i want to be backtesting like :



For the past 3 months or 1 year or 5 year (i have upto 5 years data) - run this strategy where we buy when ema5>ema21 and orderbook spread is small and volume is rising and exit when ema21>ema5 or at end of day.



In these the strike we buy is one which is ATM or OTM or ITM - 100 points



Sometimes i would want to do, only do testing for above in noon times from 12pm - 2 pm or only for wednesday or expiry day (tue/thursday) only

This specific workflow‚Äîdynamic strike selection (ATM/OTM) and conditional filtering (Wednesdays/Expiry only)‚Äîis exactly why the Daily Partitioned structure is mandatory.

If you keep "One File Per Token," you cannot find "ATM" because you don't know which file represents ATM without opening all of them to check the price.

With the Daily Partitioned structure (repacked as discussed previously), your backtesting loop becomes incredibly efficient.

Here is the blueprint for running that exact strategy.

1. The Missing Piece: The "Spot" Price
To calculate "ATM" or "EMA of the Index," you need the underlying index data (NIFTY 50 or NIFTY BANK) loaded alongside your options.

Ensure your repacker also creates a partition for the index itself: /data/date=2025-01-27/underlying=INDEX/NIFTY_BANK.parquet

2. The Backtest Logic (Polars)
This script demonstrates how to load one day, calculate the Spot EMA, find the dynamic strike (Spot - 100), and check the spread.

Python
import polars as pl
from datetime import datetime, time

def run_backtest_for_day(date_path, index_path, strategy_params):
    # 1. Load the Index (Spot) Data for this day
    # We need this to calculate EMAs and define "ATM"
    df_spot = pl.read_parquet(index_path)
    
    # RESAMPLE SPOT TO 1-MINUTE CANDLES (Standard for EMA strategies)
    # running EMAs on raw ticks is too noisy and slow
    spot_candles = (
        df_spot.sort("timestamp")
        .group_by_dynamic("timestamp", every="1m")
        .agg([
            pl.col("price").last().alias("close"),
        ])
        .with_columns([
            pl.col("close").ewm(span=5).mean().alias("ema5"),
            pl.col("close").ewm(span=21).mean().alias("ema21"),
        ])
    )

    # 2. Identify the "Entry Time" based on Spot Condition
    # Find timestamps where EMA5 crosses above EMA21
    # and Time is between 12pm - 2pm (if required)
    entry_signals = spot_candles.filter(
        (pl.col("ema5") > pl.col("ema21")) &
        (pl.col("ema5").shift(1) <= pl.col("ema21").shift(1)) & # Crossover check
        (pl.col("timestamp").dt.time().is_between(time(12,0), time(14,0)))
    )

    if entry_signals.height == 0:
        return None # No trade today

    # 3. Load the Options Data for this day
    # We lazy scan because we might filter heavily before reading
    q_opts = pl.scan_parquet(date_path)

    # 4. Filter for Specific Days (e.g., Expiry Only) if requested
    # We check if the 'date' matches the 'expiry_date' column we created during repack
    if strategy_params['expiry_day_only']:
        # We assume the first row's date is the date for the file
        current_date = df_spot['timestamp'][0].date()
        q_opts = q_opts.filter(pl.col("expiry_date") == current_date)

    # 5. Execute Strategy for each Signal
    results = []
    
    for row in entry_signals.iter_rows(named=True):
        signal_time = row['timestamp']
        spot_price = row['close']
        
        # DYNAMIC STRIKE SELECTION
        # Logic: "ITM - 100" means if Spot is 45000, we want 44900 Call
        target_strike = round((spot_price - 100) / 100) * 100 
        
        # Filter Options Data for this specific moment + strike
        # We look for data "around" the signal time (e.g., next 1 second tick)
        trade_data = (
            q_opts
            .filter(
                (pl.col("strike") == target_strike) & 
                (pl.col("option_type") == "CE") &
                (pl.col("timestamp") >= signal_time)
            )
            .head(1) # Get the very first tick after signal
            .collect()
        )
        
        if trade_data.height > 0:
            tick = trade_data.row(0, named=True)
            
            # CHECK SPREAD & VOLUME CONDITION
            spread = tick['sp0'] - tick['bp0'] # Ask - Bid
            
            if spread < strategy_params['max_spread']:
                # RECORD TRADE
                results.append({
                    "entry_time": tick['timestamp'],
                    "strike": tick['strike'],
                    "entry_price": tick['sp0'], # Buy at Ask
                    "spot_at_entry": spot_price
                })
                # ... Add exit logic here ...

    return results
3. How this handles your specific needs
A. "Past 5 Years"
You wrap the function above in a simple loop. Since each day is a separate folder, your RAM never fills up. You process 2020-01-01, clear RAM, process 2020-01-02, etc.

B. "Expiry Day Logic (Tue vs Thu)"
Notice step 4 in the code: q_opts.filter(pl.col("expiry_date") == current_date) Because we repacked the data with an explicit expiry_date column, you don't need to write if year < 2023: check Thursday else check Tuesday. The data knows when it expires.

C. "Strike Selection (ITM - 100)"
Because we load the Spot data first, we can calculate target_strike = spot_price - 100 dynamically for every single minute of the last 5 years.

D. "Orderbook Spread"
Since your data includes bp0 (Bid) and sp0 (Ask), calculating spread is a simple subtraction: sp0 - bp0. You can filter trades where spread > 5.0 to avoid slippage.

4. Recommended Data Structure for Speed
To make the above code run instantly, your file hierarchy must look like this:

Plaintext
/data/
  /date=2024-02-01/
     /underlying=BANKNIFTY/
         part-0.parquet   (Contains all strikes, huge file)
     /underlying=INDEX/
         part-0.parquet   (Contains Nifty Bank Spot ticks)
Next Step: Does your raw data currently include the Index Spot Prices (e.g., NIFTY 50 index value) inside the same folders, or is that in a separate CSV/Database? We need to locate that to build the "Spot" partition.






Gemini can make mistakes, so double-check it

