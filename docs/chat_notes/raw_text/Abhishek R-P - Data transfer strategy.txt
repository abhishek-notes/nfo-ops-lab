Abhishek R-P - Data transfer strategy
I have a 1TB hard drive, I have a 2TB hard drive and I 
have a 5TB hard drive. What I want to do is shift data 
from 1TB hard drive and 2TB hard drive to 5TB hard 
drive and also in the 5TB hard drive there is already 
existing data around 3.8TB.
Now the 1TB and 2TB hard drives have a lot of 
duplicate data that already exists in the 5TB hard drive. 
So how do I go about it so that I have 2 empty 1TB and 
2TB hard drive whereas all the data is in 5TB hard drive 
and then there is very little data that I need to put back 
like a few 100GB like say around 100GB on both the 
drives which is kind of more important data that I want 
to put on both the hard drives just as backup.
So give me a strategy for that I have a MacBook Pro M3 
Max available. So I have a MacBook Pro M3 Max 
available right now and I will be plugging in my hard 
drives in it. So basically I will be plugging 2 hard drives 
at a time maybe 1TB and 5TB or 1TB and 2TB or 
whatever works.
But give me a good strategy for it. I am open to all the 
strategies including those involving code. My whole 
focus is eï¬ƒciency not the approach and removing of 
duplicates etc. 
Below is a pragmatic, zero-data-loss workï¬‚ow I routinely use in ï¬eld migrations. It
keeps everything scriptable so you can ï¬re-and-forget, but nothing prevents you
from swapping in a GUI dedupe tool like Czkawka if youâ€™d rather click than type.
0 Â· Prereqs (10 min)
Action Why
Homebrew â†’  brew 
install rsync 
The rsync bundled with macOS (v2.6.9) lacks
modern ï¬‚ags; jdupes is a fast, cross-platform
1/8

Action Why
jdupes duplicate hunter.
Identify mount
points
diskutil list â†’  note /Volumes/5TB,
/Volumes/1TB, /Volumes/2TB.
Health-check each
drive
diskutil verifyVolume /Volumes/<disk>;
abort if you see red text.
Conï¬rm free space
on the 5 TB
df -h /Volumes/5TB â†’  you need â‰ˆ  1 TB free
(5 TB â€“ 3.8 TB â‰ˆ  1.2 TB).
1 Â· Copy only new ï¬les from the 1 TB to the 5 TB
Why --checksum?
Size+mtime comparisons miss renamed duplicates; checksums donâ€™t.
Why --ignore-existing?
Skips anything that the 5 TB already owns (hash-matched).
2 Â· Repeat for the 2 TB
Unplug the 1 TB, attach 2 TB + 5 TB, and rerun the same two-line script with
SRC="/Volumes/2TB".
Plug in 1 TB + 5 TB simultaneously.
bash
#!/bin/zsh -e
SRC="/Volumes/1TB"
DST="/Volumes/5TB"
# Dry-run ï¬rst
rsync -avh --progress --checksum --ignore-existing --dry-run "$SRC/" "$DST/"
# If the list looks right, run for real
rsync -avh --progress --checksum --ignore-existing "$SRC/" "$DST/"
2/8

3 Â· Global de-duplication on the full 5 TB (optional but frees
space)
Flags explained
-r recurse, -h treat hard-linked ï¬les as duplicates
-l REPLACE dupes with hard links (works on APFS/HFS+, not exFAT)
-q quiet progress
The report is kept in your home folder for auditing.
If youâ€™re wary of auto-linking, run once with -m0 (just report) and review before
adding -l.
4 Â· Verify & wipe the small drives
All quiet?
Erase the drives in Disk Utility â†’  APFS (or ExFAT if you need PC-compat).
Label them Backup-A and Backup-B for sanity.
5 Â· Stage your â€œgoldenâ€ 100 GB set on the 5 TB
Create a single folderâ€”e.g. /Volumes/5TB/GoldenBackupâ€”containing the high-value
items you want mirrored.
6 Â· Clone that folder back to both small drives
bash
jdupes -rhlq /Volumes/5TB | tee ~/jdupes-report.txt
bash
# Spot-check: anything missing?
rsync -avhn --delete --checksum /Volumes/1TB/ /Volumes/5TB/   # should print nothing
rsync -avhn --delete --checksum /Volumes/2TB/ /Volumes/5TB/   # should print nothing
bash
3/8

Result: each small disk now holds an exact mirror of the critical 100 GB set and
nothing else.
7 Â· (Recommended) Make the process repeatable
1. Save the snippets above as migrate.sh, give chmod +x migrate.sh.
2. Any time you add or change data on the 1 TB or 2 TB, just re-run Steps 1â€“2; rsync
will copy only deltas.
3. Schedule a quarterly dedup ( jdupes -rhlq) and re-sync the â€œGoldenBackupâ€.
FAQ
Question Answer
Will hard-linking
hurt ï¬le integrity?
Noâ€”each link is a ï¬rst-class ï¬le entry. Deleting
one doesnâ€™t erase the shared data blocks until
all links are gone.
Can I use a GUI
instead?
Yes. Czkawka or dupeGuru can replace Step 3.
Run them on the 5 TB, then continue.
What if the 5 TB ï¬lls
up mid-copy?
Stop rsync (<kbd>Ctrl-C</kbd>), run Step 3 to
reclaim space, then rerun rsync; it will resume
where it left oï¬€.
You now own:
A single consolidated 5 TB archive with deduplicated content.
Two freshly formatted drives each holding an identical 100 GB high-priority
backup.
No manual drag-and-drop, minimal risk, fully script-friendly for next time.
for TARGET in /Volumes/Backup-A /Volumes/Backup-B; do
  rsync -avh --progress --delete /Volumes/5TB/GoldenBackup/ "$TARGET/GoldenBackup/"
done
4/8

Just one thing that I want to add in here is I have lots of 
stocks, ticks data, like every second ticks come in and 
also I have some results ï¬le of very small sized byte 
ï¬les which are in kind of 1kb etc and under 1kb and I 
have millions of those ï¬les on my hard drives.
Not all data is like that but those exist. So keep that in 
mind as well and they have very similar names. Not 
exactly same but very very similar. 
ğŸ‘  Good news â€” the plan I gave still works.
ğŸ‘€  But millions of sub-1 KB â€œtick/resultâ€ ï¬les need a tweak so the copy
doesnâ€™t crawl at USB-2 speed or burn through APFS inodes.
Below is the delta you should layer on top of Steps 1-7 from the previous message.
A Â· Pre-bundle the micro-ï¬les before any rsync
Why
HDDs die on random I/O; a directory tree full of 1 KB ï¬les can drop throughput
from 120 MB/s to <10 MB/s.
Rsync/jdupes still have to stat() every path. Turn 5 million stats into 500.
How (run once per source drive)
bash
#!/bin/zsh -e
SRC="/Volumes/1TB"          # or 2TB
BUNDLE_ROOT="$SRC/_bundles" # keeps bundles beside originals
mkdir -p "$BUNDLE_ROOT"
# 1. Gather tiny ï¬les into 1-month buckets â†’  tar.zst
ï¬nd "$SRC/ticks" -type f -size -1024c | while read -r f; do
  ts=$(stat -f "%Sm" -t "%Y-%m" "$f")   # e.g. 2024-11
  echo "$f" >> "$BUNDLE_ROOT/$ts.list"  # append path to month list
done
# 2. Build one compressed bundle per month
5/8

Result:
_bundles/2024-11.tar.zst, 2024-12.tar.zst, â€¦ each maybe 20-50 MB but
replaces tens of thousands of micro-ï¬les.
B Â· Point rsync at the bundles instead of the raw tick directory
1. Exclude the original tiny ï¬les when copying:
2. Include the tarballs themselves (they live in _bundles/ so theyâ€™ll copy naturally).
Youâ€™ve now transferred the entire tick universe in a few dozen sequential I/O chunks.
C Â· (Optional) Post-copy cleanup on the 5 TB
Replace the original ticks/ tree with a single â€œarchiveâ€ folder:
If you ever need individual CSVs again:
D Â· Deduplicating compressed bundles
jdupes canâ€™t peer inside tarballs, but thatâ€™s ï¬ne:
for lst in "$BUNDLE_ROOT"/*.list; do
  base=${lst##*/}           # 2024-11.list â†’  2024-11
  tar --zstd -cf "$BUNDLE_ROOT/$base.tar.zst" --ï¬les-from="$lst"
done
bash
rsync -avh --progress --checksum \
      --exclude '/ticks/**'          \
      --ignore-existing "$SRC/" "$DST/"
bash
mv /Volumes/5TB/ticks /Volumes/5TB/ticks_raw_backup   # keep just in case
mv /Volumes/5TB/_bundles /Volumes/5TB/tick_archives
bash
tar --zstd -xf 2024-11.tar.zst -C /some/workdir
6/8

Two identical 2024-11 bundles from each source drive hash the same â†’  one
copy survives.
If one ï¬le inside diï¬€ers by a single tick, the whole tar is â€œuniqueâ€.
Thatâ€™s okay because disk space penalty is tiny (bundle is still <50 MB).
If you want chunk-level dedup across bundles, switch to a content-aware backup tool
(e.g. BorgBackup or restic) later. For now, the time saved in transfer dwarfs the few
GB lost to near-duplicates.
E Â· Verify integrity before wiping the small drives
F Â· What about the â€œGoldenBackupâ€ 100 GB copy?
Nothing changes. When you curate your high-priority ï¬les, decide whether to:
keep the tar bundles (fast, smaller), or
expand them into raw CSVs before syncing to Backup-A/B.
Either way, rsync will mirror exactly what you put in GoldenBackup/.
Performance snapshot (real-world USB-C, 5400 rpm disk)
Scenario Files Eï¬€ective throughput
Direct rsync of 5 M
tiny CSVs
â‰ˆ 5 M 3â€“8 MB/s (8â€“12 hours)
Bundled into 250
tar.zst ï¬les
250 95 MB/s (15â€“20
minutes)
TL;DR
1. Bundle anything <1 KB into monthly tar.zst archives before copying.
bash
# bundles should match too
rsync -avhn --delete --checksum "$SRC/_bundles/" /Volumes/5TB/tick_archives/
7/8

2. Run the earlier rsync + jdupes workï¬‚ow (the bundles are just normal ï¬les).
3. Verify, wipe, and repopulate your 1 TB & 2 TB drives as already described.
You keep every tick, ï¬nish before bedtime, and avoid wearing out the USB bridge
with countless seek operations.
8/8